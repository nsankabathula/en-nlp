{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# 1. Basic Setup\n",
    "    a. ElasticSearch Config (ES_CONFIG)\n",
    "    b. File Path (PATH)\n",
    "    c. Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)\n",
    "from collections import namedtuple\n",
    "import spacy\n",
    "from rcare.nlp.customize_nlp import rcare\n",
    "rcare = rcare(spacy.load('en_core_web_sm'))\n",
    "nlp = rcare.nlp\n",
    "from rcare.nlp.doc2vec_rcare import SentenceDocument\n",
    "from nltk.tokenize import sent_tokenize \n",
    "\n",
    "import pandas as pd\n",
    "from rcare.readutils import DataFrameUtils as Utils\n",
    "from collections import namedtuple\n",
    "from rcare.nlp.sections_rcare import get_sections\n",
    "\n",
    "from rcare.es_helper import ESHelper\n",
    "ES_CONFIG = {\n",
    "    \"host\": [\n",
    "        {\n",
    "            \"host\": \"localhost\",                  \n",
    "            \"port\":\"9200\"\n",
    "        }\n",
    "    ],\n",
    "    \"log\": \"error\"\n",
    "};\n",
    "\n",
    "esHelper = ESHelper(ES_CONFIG)\n",
    "import pandas as pd\n",
    "'''\n",
    "from rcare.sqlutils import SqlUtils\n",
    "sqlUtils = SqlUtils(\"/home/paperspace/dev/sqllite-node/app/data/training.db\")\n",
    "path = \"/home/paperspace/dev/en-nlp/data/lexpredict-contraxsuite-samples/cca_2011_Q3/text/\"\n",
    "pd_txtFileNames = sqlUtils.select(\"select txtFileName from meta_data where useForTraining = 1 \\\n",
    "and agreementFileName like 'creditcardagreement_%' and lineCount > 200 \\\n",
    "order by agreementFileName\")\n",
    "filenames = list(Utils.flatten(pd_txtFileNames[:30].values )) \n",
    "filenames = filenames[18:19]\n",
    "#filenames[18:19]\n",
    "'''\n",
    "#PATH SETUP for files\n",
    "PATH = \"/home/paperspace/dev/en-nlp/data/demo/\"  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Function Definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Document = namedtuple('Document', 'id name filePath text sectionCount sections sentCount sents ents rank')\n",
    "Section = namedtuple('Section', 'id sectionId text sentCount index rank')\n",
    "Sentence = namedtuple('Sentence', 'id sectionId sentId text startChar endChar sectionText rank')\n",
    "Entity = namedtuple('Entity', 'sectionId text startChar endChar label')\n",
    "\n",
    "def __tokens__(self, text, spacy_obj = False):\n",
    "    text = text if (spacy_obj) else self.nlp(text) \n",
    "    tokens = []\n",
    "    for w in text:  \n",
    "        if (w.is_digit):\n",
    "            #tokens.append(\"-DIGIT-\")  \n",
    "            continue;\n",
    "        if (w.like_num):\n",
    "            #tokens.append(\"-DIGIT-\")  \n",
    "            continue;                    \n",
    "        elif (w._.is_cardinal):\n",
    "            #tokens.append(\"-CARDINAL-\") \n",
    "            continue;\n",
    "        elif (w.is_punct):\n",
    "            tokens.append(w.text)     \n",
    "            #continue;\n",
    "        elif (w.is_stop):\n",
    "            #tokens.append(\"-STOP-\")     \n",
    "            tokens.append(w.lower_)\n",
    "            continue;\n",
    "        elif (w.like_url):\n",
    "            #tokens.append(\"-URL-\")                  \n",
    "            continue;\n",
    "        elif(w.lemma_ == '-PRON-'):\n",
    "             tokens.append(w.lower_)\n",
    "        elif (w.text.strip() not in ('\\n', '$','\\n\\n' , '\\n\\n\\n', '\\n\\n\\n\\n','' ) ) :                 \n",
    "            tokens.append(w.lemma_)  \n",
    "\n",
    "    return tokens;\n",
    "    \n",
    "def get_id(item):\n",
    "    return \"{}\".format(item['name'])\n",
    "\n",
    "def prep_sent_tokenize( filenames, path, numfiles = None):\n",
    "    df_g = Utils.read_data_gen(filenames = filenames, path = path, numfiles = numfiles)\n",
    "    taggedDocs , tokens = [], []   \n",
    "    index = 0\n",
    "    docSents = []\n",
    "    docEnts = []\n",
    "    sents = []\n",
    "    for file_idx, fileData in enumerate(df_g):    \n",
    "        #text =  Utils.clean_document(fileData.text, rm_stop_words = True)\n",
    "        name = fileData.name\n",
    "        text = fileData.text   \n",
    "        filePath = \"{}{}\".format(path, name)\n",
    "        sents = sents + sent_tokenize(text)\n",
    "        \n",
    "    return sents        \n",
    "        \n",
    "def prep_training_data( filenames, path, numfiles = None):\n",
    "    df_g = Utils.read_data_gen(filenames = filenames, path = path, numfiles = numfiles)\n",
    "    taggedDocs , tokens = [], []   \n",
    "    index = 0\n",
    "    docSents = []\n",
    "    docEnts = []\n",
    "    for file_idx, fileData in enumerate(df_g):    \n",
    "        #text =  Utils.clean_document(fileData.text, rm_stop_words = True)\n",
    "        name = fileData.name\n",
    "        text = fileData.text   \n",
    "        filePath = \"{}{}\".format(path, name)\n",
    "        \n",
    "        logging.info(\"Processing {}\".format(name))        \n",
    "        sections = []\n",
    "        \n",
    "        sectionWithSent = []\n",
    "        docEnts = []\n",
    "        docSents = []\n",
    "        for sectionId, sectionText in enumerate(get_sections(text, score_threshold=0.6, nlp = nlp)):\n",
    "            tokens = []                       \n",
    "            \n",
    "            section = nlp(sectionText.strip() if (len(sectionText.strip()) > 0) else \" \")\n",
    "            sents = []\n",
    "            nsents = sent_tokenize(sectionText.strip() if (len(sectionText.strip()) > 0) else \" \")\n",
    "            \n",
    "            for sentId, sent in enumerate(nsents):                \n",
    "                sents.append(Sentence( \"{}_{}_{}\".format(name, sectionId, sentId), sectionId, sentId, sent, \n",
    "                                      -1, -1, section.text, None))\n",
    "            \n",
    "            sections.append(Section(\"{}_{}\".format(name, sectionId), \n",
    "                                    sectionId, sectionText, len(sents), \n",
    "                                    \"{}_{}\".format(name.lower(), sectionId),None ))\n",
    "            docSents = docSents + sents\n",
    "            docEnts = docEnts + \\\n",
    "            list(map(lambda ent: Entity(sectionId, ent.text, ent.start_char, ent.end_char,  ent.label_), section.ents))   \n",
    "            \n",
    "        taggedDocs.append(Document(\"{}\".format(name), name, filePath, text, len(sections), sections, len(docSents), docSents, docEnts, None) )\n",
    "    logging.info(\"prep_training_data done \")           \n",
    "    return taggedDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preps the file data for ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdocs = prep_training_data([\"demo_bokvisaplat.txt\"],  path = PATH, numfiles = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Upserts data to ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success count: 1, Failure count: 0\n"
     ]
    }
   ],
   "source": [
    " esHelper.bulk_stream_collection( pd.DataFrame( tdocs,columns = tdocs[0]._fields).to_dict(orient='records'), index = \"demo.meta\", doc_type = \"data\",  get_id = get_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Debug (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_bokvisaplat.txt 23 183\n",
      "++++++++++++++\n",
      "BBOK VISA CLASSIC & VISA PLATINUM  \n",
      "CARD 0 9\n",
      "**************\n",
      "++++++++++++++\n",
      "3. This agreement governs any account fo 1 12\n",
      "**************\n",
      "++++++++++++++\n",
      "6. You authorize us to charge to your ac 2 9\n",
      "**************\n",
      "++++++++++++++\n",
      "9. You agree to pay us within 25 days fr 3 17\n",
      "**************\n",
      "++++++++++++++\n",
      "B. CASH ADVANCES  A Finance Charge will  4 4\n",
      "**************\n",
      "++++++++++++++\n",
      "11. INTEREST RATE\n",
      "A. PURCHASES \n",
      "If your  5 6\n",
      "**************\n",
      "++++++++++++++\n",
      "B. CASH ADVANCES  If your account is sub 6 4\n",
      "**************\n",
      "++++++++++++++\n",
      "12. OTHER CHARGES â€“ In addition to the F 7 2\n",
      "**************\n",
      "++++++++++++++\n",
      "A. LATE PAYMENT FEE\n",
      "If you do not pay th 8 2\n",
      "**************\n",
      "++++++++++++++\n",
      "B. RETURN CHECK FEE\n",
      "We will assess a ret 9 2\n",
      "**************\n",
      "++++++++++++++\n",
      "C. INTERNATIONAL TRANSACTION FEE\n",
      "Foreign 10 3\n",
      "**************\n",
      "++++++++++++++\n",
      "E. All costs of collections including re 11 1\n",
      "**************\n",
      "++++++++++++++\n",
      "F. ATM USAGE FEE\n",
      "A fee may be imposed by 12 2\n",
      "**************\n",
      "++++++++++++++\n",
      "13.  Effective October 3, 2017, Federal  13 5\n",
      "**************\n",
      "++++++++++++++\n",
      "14. PAYMENTS AND CREDITS will generally  14 2\n",
      "**************\n",
      "++++++++++++++\n",
      "15. We agree at no extra cost to provide 15 6\n",
      "**************\n",
      "++++++++++++++\n",
      "16.  EMERGENCY CASH AND LOST CARD REPLAC 16 19\n",
      "**************\n",
      "++++++++++++++\n",
      "21. We are not accountable if anyone ref 17 40\n",
      "**************\n",
      "++++++++++++++\n",
      "32. We both agree that this agreement is 18 2\n",
      "**************\n",
      "++++++++++++++\n",
      "YOUR BILLING RIGHTS\n",
      "KEEP THIS NOTICE FOR 19 11\n",
      "**************\n",
      "++++++++++++++\n",
      "Your Rights and Our Responsibilities Aft 20 13\n",
      "**************\n",
      "++++++++++++++\n",
      "Special Rule for Credit Card Purchases\n",
      "I 21 3\n",
      "**************\n",
      "++++++++++++++\n",
      "  Interest Rates and Interest Charges\n",
      "Sa 22 9\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "doc = tdocs[0]\n",
    "print (doc.name, doc.sectionCount, doc.sentCount)\n",
    "\n",
    "for sec in doc.sections:\n",
    "    print(\"++++++++++++++\")\n",
    "    print (sec.text[:40], sec.sectionId, sec.sentCount)\n",
    "    print(\"**************\")\n",
    "\n",
    "#tdocs[0].sents[0].id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
