{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/lexpredict-contraxsuite-samples/agreements/\n",
      "['construction', 'credit', 'employment', 'eta_citi_2017.txt', 'severance', 'software_license']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from lexnlp.extract.rcare.readutils import DataFrameUtils as Utils\n",
    "from lexnlp.nlp.en.segments.sections import get_sections, build_section_break_features;\n",
    "from lexnlp.nlp.en.segments.pages import get_pages;\n",
    "from lexnlp.nlp.en.segments.paragraphs import get_paragraphs \n",
    "from lexnlp.nlp.en.segments.titles  import get_titles, build_document_title_features ;\n",
    "from lexnlp.nlp.en.segments.sentences import get_sentence_list\n",
    "from lexnlp.nlp.en.segments.utils import build_document_line_distribution\n",
    "import spacy;\n",
    "spacy_nlp = spacy.load('en_core_web_lg');\n",
    "from spacy import displacy\n",
    "#python -m spacy download en_core_web_lg\n",
    "#You can now load the model via spacy.load('en_core_web_lg')\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='test-parallel.log',format='%(module)s||%(funcName)s||[%(process)d:%(thread)d||%(asctime)s||%(levelname)s||%(message)s', level=logging.DEBUG,\n",
    "                    datefmt=logging.Formatter.default_time_format)\n",
    "print (Utils.DATA_PATH)\n",
    "print (Utils.DATA_DIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils.DATA_DIRS\n",
    "path = 'data/lexpredict-contraxsuite-samples/agreements/software_license/'\n",
    "df1 = Utils.read_csv(path = path, nrows = 10);\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0;\n",
    "def get_f(text):\n",
    "    f = get_features(text)        \n",
    "    return pd.Series(f.to_dict()) \n",
    "\n",
    "def function_on_df(df):\n",
    "    df[['regulation', 'amounts', 'citations', 'conditions', 'constraints', \n",
    "     'dates', 'definitions', 'durations', 'organizations', 'stfd_persons', 'nltk_persons', 'nltk_geopolitical', \n",
    "     'nltk_companies', 'nltk_parties', 'nltk_re_companies', 'address', 'titles']] = df.data.apply(get_f);\n",
    "    return df;\n",
    "\n",
    "\n",
    "df1 = Utils.parallelize_dataframe(df1, function_on_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df1.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in Utils.DATA_DIRS:\n",
    "    dir_path = Utils.DATA_PATH + d;    \n",
    "    print(dir_path)\n",
    "    if(d != 'all'):\n",
    "        #pass;\n",
    "        df = Utils.read_data(path = dir_path)\n",
    "        df.to_csv(dir_path+ '/'+'001.txt',header = True, index  = False , doublequote = True, sep = '|', escapechar=' ');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (Utils.DATA_PATH ) \n",
    "print (Utils.DATA_DIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = Utils.read_data(['eta_citi_2017.txt', 'joyee-ppapadim.txt'])\n",
    "file_text  = str(df.data[0])\n",
    "len(file_text.splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pages = list(get_pages(file_text))\n",
    "\n",
    "sections = []\n",
    "titles = []\n",
    "'''\n",
    "for page in pages:\n",
    "    try:\n",
    "        sections = sections + list(get_sections(page, window_pre = 0 , window_post = 0))                    \n",
    "    except:\n",
    "        sections.append(page)\n",
    "    try:\n",
    "        titles = titles + list(get_titles(page,  score_threshold = 0.1))\n",
    "    except:\n",
    "        pass\n",
    "'''\n",
    "sections = list(get_sections(file_text, 2, 4))\n",
    "paragraphs = list(get_paragraphs(file_text))\n",
    "'''\n",
    "for section in sections: \n",
    "    try:\n",
    "        paragraphs = paragraphs + list(get_paragraphs(section))\n",
    "    except:\n",
    "        paragraphs.append(section)\n",
    "'''\n",
    "sentences = get_sentence_list(file_text);\n",
    "\n",
    "#titles = list(get_titles(file_text, score_threshold = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages 38\n",
      "sections 59\n",
      "paragraphs 105\n",
      "sentences 281\n"
     ]
    }
   ],
   "source": [
    "print ('pages', len(pages));\n",
    "print ('sections', len(sections));\n",
    "print ('paragraphs', len(paragraphs));\n",
    "print ('sentences', len(sentences))\n",
    "\n",
    "#paragraphs[4]\n",
    "#pages[1].splitlines()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en(paragraphs)\n",
    "\n",
    "text = file_text#sections[2]\n",
    "#text\n",
    "t_sections = list(get_sections(text, debug = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for x in t_sections:\n",
    "    test_feature_df = x;\n",
    "'''    \n",
    "test_feature_df = t_sections[0]\n",
    "test_predicted_lines = t_sections[1]\n",
    "predicted_df = t_sections[2]\n",
    "section_breaks = t_sections[3]\n",
    "SECTION_SEGMENTER_MODEL = t_sections[4]\n",
    "#test_feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARTICLE</th>\n",
       "      <th>Article</th>\n",
       "      <th>SECTION</th>\n",
       "      <th>Section</th>\n",
       "      <th>article</th>\n",
       "      <th>char_</th>\n",
       "      <th>char_</th>\n",
       "      <th>char_</th>\n",
       "      <th>char_</th>\n",
       "      <th>char_</th>\n",
       "      <th>...</th>\n",
       "      <th>line_upper_case_-1</th>\n",
       "      <th>line_upper_case_-2</th>\n",
       "      <th>line_upper_case_-3</th>\n",
       "      <th>line_upper_case_0</th>\n",
       "      <th>line_upper_case_1</th>\n",
       "      <th>line_upper_case_2</th>\n",
       "      <th>line_upper_case_3</th>\n",
       "      <th>section</th>\n",
       "      <th>sw_article</th>\n",
       "      <th>sw_section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows  369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ARTICLE  Article  SECTION  Section  article  char_\\t  char_\\n  char_\n",
       "  \\\n",
       "0          0        0        0        0        0        0        0       0   \n",
       "1          0        0        0        0        0        0        0       0   \n",
       "2          0        0        0        1        0        0        0       0   \n",
       "3          0        0        0        0        0        0        0       0   \n",
       "4          0        0        0        0        0        0        0       0   \n",
       "5          0        0        0        0        0        0        0       0   \n",
       "6          0        0        0        0        0        0        0       0   \n",
       "7          0        0        0        1        0        0        0       0   \n",
       "8          0        0        0        0        0        0        0       0   \n",
       "9          0        0        0        0        0        0        0       0   \n",
       "10         0        0        0        0        0        0        0       0   \n",
       "11         0        0        0        0        0        0        0       0   \n",
       "12         0        0        0        0        0        0        0       0   \n",
       "13         0        0        0        0        0        0        0       0   \n",
       "14         0        0        0        0        0        0        0       0   \n",
       "15         0        0        0        0        0        0        0       0   \n",
       "16         0        0        0        0        0        0        0       0   \n",
       "17         0        0        0        0        0        0        0       0   \n",
       "18         0        0        0        0        0        0        0       0   \n",
       "19         0        0        0        0        0        0        0       0   \n",
       "20         0        0        0        0        0        0        0       0   \n",
       "21         0        0        0        0        0        0        0       0   \n",
       "22         0        0        0        0        0        0        0       0   \n",
       "23         0        0        0        0        0        0        0       0   \n",
       "24         0        0        0        0        0        0        0       0   \n",
       "25         0        0        0        0        0        0        0       0   \n",
       "26         0        0        0        0        0        0        0       0   \n",
       "27         0        0        0        0        0        0        0       0   \n",
       "28         0        0        0        0        0        0        0       0   \n",
       "29         0        0        0        0        0        0        0       0   \n",
       "..       ...      ...      ...      ...      ...      ...      ...     ...   \n",
       "475        0        0        0        0        0        0        0       0   \n",
       "476        0        0        0        0        0        0        0       0   \n",
       "477        0        0        0        0        0        0        0       0   \n",
       "478        0        0        0        0        0        0        0       0   \n",
       "479        0        0        0        0        0        0        0       0   \n",
       "480        0        0        0        0        0        0        0       0   \n",
       "481        0        0        0        0        0        0        0       0   \n",
       "482        0        0        0        0        0        0        0       0   \n",
       "483        0        0        0        0        0        1        0       0   \n",
       "484        0        0        0        0        0        0        0       0   \n",
       "485        0        0        0        0        0        0        0       0   \n",
       "486        0        0        0        1        0        0        0       0   \n",
       "487        0        0        0        0        0        0        0       0   \n",
       "488        0        0        0        0        0        0        0       0   \n",
       "489        0        0        0        0        0        0        0       0   \n",
       "490        0        0        0        0        0        0        0       0   \n",
       "491        0        0        0        0        0        0        0       0   \n",
       "492        0        0        0        0        0        0        0       0   \n",
       "493        0        0        0        0        0        0        0       0   \n",
       "494        0        0        0        0        0        0        0       0   \n",
       "495        0        0        0        0        0        0        0       0   \n",
       "496        0        0        0        0        0        0        0       0   \n",
       "497        0        0        0        0        0        0        0       0   \n",
       "498        0        0        0        0        0        0        0       0   \n",
       "499        0        0        0        0        0        0        0       0   \n",
       "500        0        0        0        0        0        0        0       0   \n",
       "501        0        0        0        0        0        0        0       0   \n",
       "502        0        0        0        0        0        0        0       0   \n",
       "503        0        0        0        0        0        0        0       0   \n",
       "504        0        0        0        0        0        0        0       0   \n",
       "\n",
       "     char_\n",
       "  char_\\r     ...      line_upper_case_-1  line_upper_case_-2  \\\n",
       "0         0        0     ...                      -1                  -1   \n",
       "1         0        0     ...                    True                  -1   \n",
       "2         0        0     ...                    True                True   \n",
       "3         0        0     ...                   False                True   \n",
       "4         0        0     ...                    True               False   \n",
       "5         0        0     ...                    True                True   \n",
       "6         0        0     ...                   False                True   \n",
       "7         0        0     ...                    True               False   \n",
       "8         0        0     ...                   False                True   \n",
       "9         0        0     ...                   False               False   \n",
       "10        0        0     ...                   False               False   \n",
       "11        0        0     ...                   False               False   \n",
       "12        0        0     ...                   False               False   \n",
       "13        0        0     ...                   False               False   \n",
       "14        0        0     ...                    True               False   \n",
       "15        0        0     ...                   False                True   \n",
       "16        0        0     ...                   False               False   \n",
       "17        0        0     ...                   False               False   \n",
       "18        0        0     ...                   False               False   \n",
       "19        0        0     ...                   False               False   \n",
       "20        0        0     ...                   False               False   \n",
       "21        0        0     ...                   False               False   \n",
       "22        0        0     ...                   False               False   \n",
       "23        0        0     ...                   False               False   \n",
       "24        0        0     ...                   False               False   \n",
       "25        0        0     ...                   False               False   \n",
       "26        0        0     ...                   False               False   \n",
       "27        0        0     ...                   False               False   \n",
       "28        0        0     ...                   False               False   \n",
       "29        0        0     ...                   False               False   \n",
       "..      ...      ...     ...                     ...                 ...   \n",
       "475       0        0     ...                    True               False   \n",
       "476       0        0     ...                    True                True   \n",
       "477       0        0     ...                    True                True   \n",
       "478       0        0     ...                   False                True   \n",
       "479       0        0     ...                    True               False   \n",
       "480       0        0     ...                   False                True   \n",
       "481       0        0     ...                    True               False   \n",
       "482       0        0     ...                   False                True   \n",
       "483       0        0     ...                    True               False   \n",
       "484       0        0     ...                    True                True   \n",
       "485       0        0     ...                    True                True   \n",
       "486       0        0     ...                    True                True   \n",
       "487       0        0     ...                   False                True   \n",
       "488       0        0     ...                    True               False   \n",
       "489       0        0     ...                   False                True   \n",
       "490       0        0     ...                    True               False   \n",
       "491       0        0     ...                    True                True   \n",
       "492       0        0     ...                   False                True   \n",
       "493       0        0     ...                    True               False   \n",
       "494       0        0     ...                    True                True   \n",
       "495       0        0     ...                   False                True   \n",
       "496       0        0     ...                    True               False   \n",
       "497       0        0     ...                    True                True   \n",
       "498       0        0     ...                   False                True   \n",
       "499       0        0     ...                    True               False   \n",
       "500       0        0     ...                    True                True   \n",
       "501       0        0     ...                    True                True   \n",
       "502       0        0     ...                    True                True   \n",
       "503       0        0     ...                    True                True   \n",
       "504       0        0     ...                    True                True   \n",
       "\n",
       "     line_upper_case_-3  line_upper_case_0  line_upper_case_1  \\\n",
       "0                    -1               True               True   \n",
       "1                    -1               True              False   \n",
       "2                    -1              False               True   \n",
       "3                  True               True               True   \n",
       "4                  True               True              False   \n",
       "5                 False              False               True   \n",
       "6                  True               True              False   \n",
       "7                  True              False              False   \n",
       "8                 False              False              False   \n",
       "9                  True              False              False   \n",
       "10                False              False              False   \n",
       "11                False              False              False   \n",
       "12                False              False               True   \n",
       "13                False               True              False   \n",
       "14                False              False              False   \n",
       "15                False              False              False   \n",
       "16                 True              False              False   \n",
       "17                False              False              False   \n",
       "18                False              False              False   \n",
       "19                False              False              False   \n",
       "20                False              False              False   \n",
       "21                False              False              False   \n",
       "22                False              False              False   \n",
       "23                False              False              False   \n",
       "24                False              False              False   \n",
       "25                False              False              False   \n",
       "26                False              False              False   \n",
       "27                False              False              False   \n",
       "28                False              False               True   \n",
       "29                False               True              False   \n",
       "..                  ...                ...                ...   \n",
       "475                True               True               True   \n",
       "476               False               True              False   \n",
       "477                True              False               True   \n",
       "478                True               True              False   \n",
       "479                True              False               True   \n",
       "480               False               True              False   \n",
       "481                True              False               True   \n",
       "482               False               True               True   \n",
       "483                True               True               True   \n",
       "484               False               True               True   \n",
       "485                True               True              False   \n",
       "486                True              False               True   \n",
       "487                True               True              False   \n",
       "488                True              False               True   \n",
       "489               False               True               True   \n",
       "490                True               True              False   \n",
       "491               False              False               True   \n",
       "492                True               True               True   \n",
       "493                True               True              False   \n",
       "494               False              False               True   \n",
       "495                True               True               True   \n",
       "496                True               True              False   \n",
       "497               False              False               True   \n",
       "498                True               True               True   \n",
       "499                True               True               True   \n",
       "500               False               True               True   \n",
       "501                True               True               True   \n",
       "502                True               True               True   \n",
       "503                True               True               True   \n",
       "504                True               True                 -1   \n",
       "\n",
       "     line_upper_case_2  line_upper_case_3  section  sw_article  sw_section  \n",
       "0                False               True        0           0           0  \n",
       "1                 True               True        0           0           0  \n",
       "2                 True              False        0           0           0  \n",
       "3                False               True        0           0           0  \n",
       "4                 True              False        0           0           0  \n",
       "5                False              False        0           0           0  \n",
       "6                False              False        0           0           0  \n",
       "7                False              False        0           0           0  \n",
       "8                False              False        0           0           0  \n",
       "9                False              False        0           0           0  \n",
       "10               False               True        0           0           0  \n",
       "11                True              False        0           0           0  \n",
       "12               False              False        0           0           0  \n",
       "13               False              False        0           0           0  \n",
       "14               False              False        0           0           0  \n",
       "15               False              False        0           0           0  \n",
       "16               False              False        0           0           0  \n",
       "17               False              False        0           0           0  \n",
       "18               False              False        0           0           0  \n",
       "19               False              False        0           0           0  \n",
       "20               False              False        0           0           0  \n",
       "21               False              False        0           0           0  \n",
       "22               False              False        0           0           0  \n",
       "23               False              False        0           0           0  \n",
       "24               False              False        0           0           0  \n",
       "25               False              False        0           0           0  \n",
       "26               False               True        0           0           0  \n",
       "27                True              False        0           0           0  \n",
       "28               False              False        0           0           0  \n",
       "29               False              False        0           0           0  \n",
       "..                 ...                ...      ...         ...         ...  \n",
       "475              False               True        0           0           0  \n",
       "476               True              False        0           0           0  \n",
       "477              False               True        0           0           0  \n",
       "478               True              False        0           0           0  \n",
       "479              False               True        0           0           0  \n",
       "480               True               True        0           0           0  \n",
       "481               True               True        0           0           0  \n",
       "482               True               True        0           0           0  \n",
       "483               True              False        0           0           0  \n",
       "484              False               True        0           0           0  \n",
       "485               True              False        0           0           0  \n",
       "486              False               True        0           0           0  \n",
       "487               True               True        0           0           0  \n",
       "488               True              False        0           0           0  \n",
       "489              False               True        0           0           0  \n",
       "490               True               True        0           0           0  \n",
       "491               True              False        0           0           0  \n",
       "492              False               True        0           0           0  \n",
       "493               True               True        0           0           0  \n",
       "494               True              False        0           0           0  \n",
       "495              False               True        0           0           0  \n",
       "496               True               True        0           0           0  \n",
       "497               True               True        0           0           0  \n",
       "498               True               True        0           0           0  \n",
       "499               True               True        0           0           0  \n",
       "500               True               True        0           0           0  \n",
       "501               True               True        0           0           0  \n",
       "502               True                 -1        0           0           0  \n",
       "503                 -1                 -1        0           0           0  \n",
       "504                 -1                 -1        0           0           0  \n",
       "\n",
       "[505 rows x 369 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_feature_df[test_feature_df['first_char_number'] == True]\n",
    "#len(list(test_feature_df.columns))\n",
    "#test_feature_df\n",
    "test_feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.concat([\n",
    "    pd.DataFrame(list(test_feature_df.columns), columns= ['feature']),\n",
    "    pd.DataFrame(list(SECTION_SEGMENTER_MODEL.feature_importances_) , columns= ['importance'])\n",
    "], axis=1) \n",
    "feature_importance_df[feature_importance_df.feature == 'line_n_punct_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt = sections[2]\n",
    "#df = build_document_title_features(file_text)\n",
    "#df\n",
    "import unicodedata\n",
    "lines = text.splitlines();\n",
    "feature_vector = {}\n",
    "for line_id in range(len(lines)):\n",
    "    line_window_pre = 3\n",
    "    line_window_post = 3\n",
    "    if line_id < line_window_pre:\n",
    "        line_window_pre = line_id\n",
    "\n",
    "    # Check final offset\n",
    "    if (line_id + line_window_post) >= len(lines):\n",
    "        line_window_post = len(lines) - line_window_post - 1\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range(-line_window_pre, line_window_post + 1):\n",
    "        #print (line_id + i)\n",
    "        try:\n",
    "            line = lines[line_id + i]\n",
    "            feature_vector[\"line_n_punct_{0}\".format(i)] = sum([1 for c in line if unicodedata.category(c).startswith(\"P\")])\n",
    "        except: \n",
    "            continue\n",
    "\n",
    "#SECTION_SEGMENTER_MODEL.n_features_  \n",
    "feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sections[3]\n",
    "#paragraphs[5]\n",
    "pages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_docs (docs):\n",
    "    s_docs = [];\n",
    "    for doc in docs:\n",
    "        s_docs.append(spacy_nlp(doc))\n",
    "    \n",
    "    return s_docs\n",
    "\n",
    "def display_ents(spacy_doc, nrows = 10, filter_labels = ['ORG','CARDINAL', 'PERSON', 'ORDINAL', 'PRODUCT']):\n",
    "    i = 0;\n",
    "    for ent in spacy_doc.ents:\n",
    "        if(ent.label_ in filter_labels)\n",
    "            print (ent.text, ent.label_, ent.start_char, ent.end_char)\n",
    "            i = i + 1;\n",
    "            if(i> nrows): break;\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy_pgraphs = spacy_docs(paragraphs)\n",
    "#spacy_sections = spacy_docs(sections)\n",
    "spacy_sentences = spacy_docs(sentences)\n",
    "spacy_doc = spacy_docs([df.data[0], df.data[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy_sections[1].ents\n",
    "import string\n",
    "string.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy_doc = spacy_docs([file_text, \"When I told John that I wanted to move to Alaska, he warned me that I'd have trouble finding a Starbucks there.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9517414296933956"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display_ents(spacy_doc[0], 50)\n",
    "spacy_doc[0].similarity  (spacy_doc[1])\n",
    "#spacy_doc[0]\n",
    "\n",
    "\n",
    "#displacy.render(spacy_doc[0], style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sents0 = list(spacy_doc[0].sents)\n",
    "sents1 = list(spacy_doc[1].sents)\n",
    "\n",
    "similarity = [.0] * len(sents0)\n",
    "for i in range(len(sents0)):\n",
    "    doc_i = sents0[i]\n",
    "    similarity[i] = [.0] *len(sents1)\n",
    "    for j in range(len(sents1)):\n",
    "        doc_j = sents1[j]\n",
    "        similarity[i][j] = doc_i.similarity(doc_j)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You) and (2) the undersigned Citi entity, on behalf of itself and each of the other Citi entities defined in Section 8 (hereafter Citi). \n",
      "XXX\n",
      " In par-\n",
      "ticular, we study structured documents that describe food\n",
      "items and contain attributes such as name, brand, category,\n",
      "ingredients, nutrients, etc.\n",
      "0.7953381175476241\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "j = 3\n",
    "print (sents0[i],'\\nXXX\\n' ,sents1[j])\n",
    "print ( similarity[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ELECTRONIC ACCESS AND TRADING AGREEMENT\n",
      " \n",
      " \n",
      "1 This ELECTRONIC ACCESS AND TRADING AGREEMENT (together with any Exhibits, Schedules and Addenda hereto which are hereby incorporated herein by reference, the Agreement) is between (1) the client identified on the signature page below (Client or  \n",
      "2 You) and (2) the undersigned Citi entity, on behalf of itself and each of the other Citi entities defined in Section 8 (hereafter Citi). \n",
      "3 This Agreement sets forth the terms and conditions under which you may be permitted to use the products and services offered on the System (as defined below).   \n",
      "4 The system shall include all order and related messages, research, market commentary, data, (including Market Data (as defined below)), analytical tools, software, video, audio, graphics, pricing information and other content (collectively System Content) transmitted via (i) a FIX connection, (ii) the internet, (iii) front-end systems or software whether provided by Citi or a third-party (which may include proprietary or Citi algorithms embedded therein); (iv) front-end order routing facility provided by Citi or a third-party (which may include proprietary or Citi algorithms embedded therein), (v) telephone and other forms of electronic messaging, or (vi) as otherwise notified by Citi (collectively System). \n",
      "5 Clients may enter orders for certain financial securities, commodities, currencies, options, derivatives and other financial instruments (collectively, Products) into the System from Clients trading facility and telecommunication or interface or otherwise (such permitted orders collectively referred to as Orders) for execution and/or clearance by Citi in each case for routing to various execution venues, as applicable.   \n",
      "6 Client agrees that this Agreement and any Instructions (as defined below) are deemed to be in writing and to have been executed for all purposes and will have the same legally binding nature, validity and enforceability, as if each were originated, executed and maintained in paper form.   \n",
      "7 Citi will give you prior notice of any material amendment to this Agreement that affects you. \n",
      "8 Unless immediately followed by the words as an individual user, the term you refers to both you as an individual user of the System and Client. \n",
      "9 The Agreement shall commence on the later of the two signature dates on the signature page below (the Effective Date). \n",
      "\n",
      " \n",
      "10 8\n",
      "December 2017 \n",
      "11 Version \n",
      "\n",
      "1.   \n",
      "12 System Use. \n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for token in spacy_doc[0].sents:\n",
    "    print (i, token.text, token.label_)\n",
    "    i = i + 1\n",
    "    if(i > 12): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '_py_tokens',\n",
       " '_realloc',\n",
       " '_vector',\n",
       " '_vector_norm',\n",
       " 'cats',\n",
       " 'char_span',\n",
       " 'count_by',\n",
       " 'doc',\n",
       " 'ents',\n",
       " 'extend_tensor',\n",
       " 'from_array',\n",
       " 'from_bytes',\n",
       " 'from_disk',\n",
       " 'get_extension',\n",
       " 'get_lca_matrix',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'is_parsed',\n",
       " 'is_sentenced',\n",
       " 'is_tagged',\n",
       " 'mem',\n",
       " 'merge',\n",
       " 'noun_chunks',\n",
       " 'noun_chunks_iterator',\n",
       " 'print_tree',\n",
       " 'retokenize',\n",
       " 'sentiment',\n",
       " 'sents',\n",
       " 'set_extension',\n",
       " 'similarity',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'to_array',\n",
       " 'to_bytes',\n",
       " 'to_disk',\n",
       " 'user_data',\n",
       " 'user_hooks',\n",
       " 'user_span_hooks',\n",
       " 'user_token_hooks',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA = [\n",
    "     (\"Uber blew through $1 million a week\", {'entities': [(0, 4, 'ORG')]}),\n",
    "     (\"Google rebrands its business apps\", {'entities': [(0, 6, \"ORG\")]})]\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(20):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        nlp.update([text], [annotations], sgd=optimizer)\n",
    "nlp.to_disk('/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 CARDINAL 0 1\n",
      "15578876784678163569 HelloWorld 0 2 1.\n",
      "15578876784678163569 HelloWorld 0 2 1.\n",
      "15578876784678163569 HelloWorld 3 5 A:\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# add match ID \"HelloWorld\" with no callback and one pattern\n",
    "\n",
    "matcher.add('HelloWorld', None, [{'ENT_TYPE': 'CARDINAL'}, {'IS_PUNCT': True}],[{'LENGTH': 1}, {'IS_PUNCT': True}] )\n",
    "\n",
    "doc = nlp('1. Section A: This is section. a. Section B')\n",
    "matches = matcher(doc)\n",
    "display_ents(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = doc[start:end]  # the matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13110060611322374290 xxxx 10716 10717 DMA\n",
      "13110060611322374290 xxxx 10748 10749 DMA\n",
      "13110060611322374290 xxxx 10766 10767 DMA\n",
      "13110060611322374290 xxxx 10879 10880 DMA\n",
      "13110060611322374290 xxxx 10919 10920 DMA\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{'ORTH': 'DMA'}]\n",
    "matcher.add('xxxx', None, pattern)\n",
    "\n",
    "doc = nlp(file_text)\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = doc[start:end]  # the matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardinal 33 35 )\n",
      "cardinal 38 40 1)\n",
      "cardinal 48 50 (\n",
      "cardinal 55 57 )\n",
      "cardinal 59 61 2)\n",
      "cardinal 80 82 8 (\n",
      "cardinal 85 87 )\n",
      "cardinal 86 88 ).\n",
      "cardinal 116 118 ).\n",
      "cardinal 135 137 , (\n",
      "cardinal 144 146 ))\n",
      "cardinal 145 147 ),\n",
      "cardinal 168 170 )\n",
      "cardinal 173 175 i)\n",
      "cardinal 178 180 , (\n",
      "cardinal 184 186 , (\n",
      "cardinal 213 215 );\n",
      "cardinal 214 216 ; (\n",
      "cardinal 242 244 ),\n",
      "cardinal 243 245 , (\n",
      "cardinal 245 247 v)\n",
      "cardinal 268 270 )\n",
      "cardinal 269 271 ).\n",
      "cardinal 293 295 , \n",
      "cardinal 296 298 )\n",
      "cardinal 322 324 )\n",
      "cardinal 430 432 ,\n",
      "cardinal 473 475 )\n",
      "cardinal 474 476 ).\n",
      "cardinal 483 485 1.\n",
      "cardinal 483 485 1.\n",
      "cardinal 488 490 . (\n",
      "cardinal 490 492 a)\n",
      "cardinal 530 532 i)\n",
      "cardinal 538 540 );\n",
      "cardinal 539 541 ; (\n",
      "cardinal 569 571 i)\n",
      "cardinal 581 583 (\n",
      "cardinal 585 587 )\n",
      "cardinal 586 588 ),\n",
      "cardinal 587 589 , (\n",
      "cardinal 624 626 , \n",
      "cardinal 627 629 )\n",
      "cardinal 640 642 ; (\n",
      "cardinal 745 747 i)\n",
      "cardinal 800 802 \n",
      "(\n",
      "cardinal 802 804 b)\n",
      "cardinal 808 810 i)\n",
      "cardinal 816 818 (\n",
      "cardinal 819 821 )\n",
      "cardinal 820 822 ),\n",
      "cardinal 833 835 ; (\n",
      "cardinal 854 856 ; (\n",
      "cardinal 895 897 (\n",
      "cardinal 899 901 )\n",
      "cardinal 900 902 ),\n",
      "cardinal 901 903 , (\n",
      "cardinal 942 944 (\n",
      "cardinal 945 947 )\n",
      "cardinal 946 948 ),\n",
      "cardinal 950 952 v)\n",
      "cardinal 1023 1025 \n",
      "(\n",
      "cardinal 1025 1027 c)\n",
      "cardinal 1189 1191 2.\n",
      "cardinal 1189 1191 2.\n",
      "cardinal 1265 1267 , \n",
      "cardinal 1269 1271 )\n",
      "cardinal 1270 1272 ).\n",
      "cardinal 1338 1340 (\n",
      "cardinal 1341 1343 )\n",
      "cardinal 1372 1374 )\n",
      "cardinal 1489 1491 (\n",
      "cardinal 1493 1495 )\n",
      "cardinal 1494 1496 ),\n",
      "cardinal 1518 1520 3.\n",
      "cardinal 1518 1520 3.\n",
      "cardinal 1650 1652 4.\n",
      "cardinal 1650 1652 4.\n",
      "cardinal 1722 1724 ),\n",
      "cardinal 1800 1802 ).\n",
      "cardinal 1892 1894 a \n",
      "cardinal 1897 1899 )\n",
      "cardinal 1898 1900 ).\n",
      "cardinal 2168 2170 I)\n",
      "cardinal 2202 2204 , (\n",
      "cardinal 2231 2233 , (\n",
      "cardinal 2292 2294 5.\n",
      "cardinal 2292 2294 5.\n",
      "cardinal 2311 2313 \n",
      "(\n",
      "cardinal 2313 2315 a)\n",
      "cardinal 2343 2345 (\n",
      "cardinal 2346 2348 )\n",
      "cardinal 2347 2349 ),\n",
      "cardinal 2371 2373 ) (\n",
      "cardinal 2374 2376 , \n",
      "cardinal 2378 2380 )\n",
      "cardinal 2379 2381 ),\n",
      "cardinal 2522 2524 b)\n",
      "cardinal 2539 2541 ),\n",
      "cardinal 2576 2578 (\n",
      "cardinal 2579 2581 )\n",
      "cardinal 2580 2582 ).\n",
      "cardinal 2684 2686 i)\n",
      "cardinal 2691 2693 ; (\n",
      "cardinal 2730 2732 \n",
      "(\n",
      "cardinal 2732 2734 c)\n",
      "cardinal 2821 2823 \n",
      "(\n",
      "cardinal 2823 2825 d)\n",
      "cardinal 3044 3046 \n",
      "(\n",
      "cardinal 3046 3048 e)\n",
      "cardinal 3114 3116 \n",
      "(\n",
      "cardinal 3116 3118 f)\n",
      "cardinal 3146 3148 \n",
      "(\n",
      "cardinal 3148 3150 g)\n",
      "cardinal 3182 3184 \n",
      "(\n",
      "cardinal 3184 3186 h)\n",
      "cardinal 3205 3207 , (\n",
      "cardinal 3241 3243 i)\n",
      "cardinal 3300 3302 \n",
      "(\n",
      "cardinal 3302 3304 i)\n",
      "cardinal 3372 3374 ; (\n",
      "cardinal 3374 3376 a)\n",
      "cardinal 3430 3432 ; (\n",
      "cardinal 3432 3434 b)\n",
      "cardinal 3470 3472 c)\n",
      "cardinal 3498 3500 \n",
      "(\n",
      "cardinal 3500 3502 j)\n",
      "cardinal 3550 3552  \n",
      "cardinal 3558 3560 e-\n",
      "cardinal 3610 3612 ).\n",
      "cardinal 3647 3649 (\n",
      "cardinal 3650 3652 )\n",
      "cardinal 3651 3653 ),\n",
      "cardinal 3689 3691 \n",
      "(\n",
      "cardinal 3691 3693 k)\n",
      "cardinal 3729 3731 a \n",
      "cardinal 3733 3735  (\n",
      "cardinal 3756 3758 ).\n",
      "cardinal 3789 3791 (\n",
      "cardinal 3794 3796 )\n",
      "cardinal 3795 3797 ),\n",
      "cardinal 3806 3808 i)\n",
      "cardinal 3819 3821 a \n",
      "cardinal 3844 3846 a \n",
      "cardinal 3857 3859 a \n",
      "cardinal 3957 3959 \n",
      "(\n",
      "cardinal 3959 3961 l)\n",
      "cardinal 4075 4077 \n",
      "(\n",
      "cardinal 4077 4079 m)\n",
      "cardinal 4094 4096 i)\n",
      "cardinal 4099 4101 ; (\n",
      "cardinal 4131 4133 6.\n",
      "cardinal 4131 4133 6.\n",
      "cardinal 4215 4217 A \n",
      "cardinal 4247 4249 7.\n",
      "cardinal 4247 4249 7.\n",
      "cardinal 4255 4257 \n",
      "(\n",
      "cardinal 4257 4259 a)\n",
      "cardinal 4367 4369 ) (\n",
      "cardinal 4370 4372 , \n",
      "cardinal 4373 4375 )\n",
      "cardinal 4418 4420 ),\n",
      "cardinal 4424 4426 \n",
      "(\n",
      "cardinal 4426 4428 b)\n",
      "cardinal 4448 4450 i)\n",
      "cardinal 4458 4460 , (\n",
      "cardinal 4473 4475 , (\n",
      "cardinal 4489 4491 , (\n",
      "cardinal 4505 4507 v)\n",
      "cardinal 4595 4597 \n",
      "(\n",
      "cardinal 4597 4599 c)\n",
      "cardinal 4628 4630 i)\n",
      "cardinal 4731 4733 \n",
      "(\n",
      "cardinal 4733 4735 d)\n",
      "cardinal 4767 4769 8.\n",
      "cardinal 4779 4781 , \n",
      "cardinal 4870 4872 .\n",
      "cardinal 4873 4875 9.\n",
      "cardinal 4873 4875 9.\n",
      "cardinal 4961 4963 i)\n",
      "cardinal 4977 4979 , (\n",
      "cardinal 5024 5026 ).\n",
      "cardinal 5090 5092 a)\n",
      "cardinal 5104 5106 , (\n",
      "cardinal 5106 5108 b)\n",
      "cardinal 5124 5126 , (\n",
      "cardinal 5126 5128 c)\n",
      "cardinal 5143 5145 d)\n",
      "cardinal 5208 5210 , (\n",
      "cardinal 5210 5212 i)\n",
      "cardinal 5262 5264 ).\n",
      "cardinal 5264 5266 \n",
      "[\n",
      "cardinal 5267 5269 : _\n",
      "cardinal 5293 5295 ),\n",
      "cardinal 5325 5327 , \n",
      "cardinal 5329 5331 )\n",
      "cardinal 5330 5332 ).\n",
      "cardinal 5359 5361 (\n",
      "cardinal 5363 5365 )\n",
      "cardinal 5528 5530 : (\n",
      "cardinal 5530 5532 i)\n",
      "cardinal 5541 5543 ; (\n",
      "cardinal 5553 5555 ; (\n",
      "cardinal 5562 5564 ; (\n",
      "cardinal 5574 5576 ; (\n",
      "cardinal 5576 5578 v)\n",
      "cardinal 5676 5678 2,\n",
      "cardinal 5676 5678 2,\n",
      "cardinal 5678 5680 3,\n",
      "cardinal 5678 5680 3,\n",
      "cardinal 5680 5682 4,\n",
      "cardinal 5682 5684 5,\n",
      "cardinal 5684 5686 6,\n",
      "cardinal 5686 5688 7,\n",
      "cardinal 5688 5690 9,\n",
      "cardinal 5743 5745 12.\n",
      "cardinal 5944 5946 , (\n",
      "cardinal 5953 5955 ),\n",
      "cardinal 5978 5980 13.\n",
      "cardinal 6123 6125 : (\n",
      "cardinal 6125 6127 a)\n",
      "cardinal 6135 6137 ),\n",
      "cardinal 6142 6144 ; (\n",
      "cardinal 6144 6146 b)\n",
      "cardinal 6163 6165 c)\n",
      "cardinal 6181 6183 14.\n",
      "cardinal 6211 6213 : (\n",
      "cardinal 6213 6215 a)\n",
      "cardinal 6252 6254 b)\n",
      "cardinal 6367 6369 (\n",
      "cardinal 6371 6373 )\n",
      "cardinal 6372 6374 ).\n",
      "cardinal 6437 6439 1.\n",
      "cardinal 6487 6489 , \n",
      "cardinal 6490 6492 )\n",
      "cardinal 6491 6493 ),\n",
      "cardinal 6584 6586 2.\n",
      "cardinal 6611 6613 (\n",
      "cardinal 6614 6616 )\n",
      "cardinal 6615 6617 ),\n",
      "cardinal 6634 6636 f/\n",
      "cardinal 6636 6638 k/\n",
      "cardinal 6651 6653 ),\n",
      "cardinal 6760 6762 3.\n",
      "cardinal 6852 6854 4.\n",
      "cardinal 6852 6854 4.\n",
      "cardinal 6949 6951 5.\n",
      "cardinal 6949 6951 5.\n",
      "cardinal 7011 7013 6.\n",
      "cardinal 7090 7092 7.\n",
      "cardinal 7090 7092 7.\n",
      "cardinal 7127 7129 ) (\n",
      "cardinal 7133 7135 )\n",
      "cardinal 7134 7136 ),\n",
      "cardinal 7262 7264 ),\n",
      "cardinal 7360 7362 8.\n",
      "cardinal 7360 7362 8.\n",
      "cardinal 7366 7368 a)\n",
      "cardinal 7383 7385 ,\n",
      "cardinal 7402 7404 ,\n",
      "cardinal 7419 7421 b)\n",
      "cardinal 7477 7479 (\n",
      "cardinal 7480 7482 )\n",
      "cardinal 7628 7630 1.\n",
      "cardinal 7658 7660 (\n",
      "cardinal 7663 7665 )\n",
      "cardinal 8135 8137 a)\n",
      "cardinal 8156 8158 i)\n",
      "cardinal 8174 8176 );\n",
      "cardinal 8177 8179 \n",
      "(\n",
      "cardinal 8206 8208 \n",
      "(\n",
      "cardinal 8243 8245 b)\n",
      "cardinal 8269 8271 (\n",
      "cardinal 8272 8274 )\n",
      "cardinal 8281 8283 (\n",
      "cardinal 8286 8288 )\n",
      "cardinal 8287 8289 ),\n",
      "cardinal 8294 8296 (\n",
      "cardinal 8297 8299 )\n",
      "cardinal 8298 8300 ),\n",
      "cardinal 8303 8305 (\n",
      "cardinal 8306 8308 )\n",
      "cardinal 8307 8309 ),\n",
      "cardinal 8313 8315 (\n",
      "cardinal 8316 8318 )\n",
      "cardinal 8317 8319 ),\n",
      "cardinal 8322 8324 (\n",
      "cardinal 8325 8327 )\n",
      "cardinal 8326 8328 ),\n",
      "cardinal 8331 8333 (\n",
      "cardinal 8334 8336 )\n",
      "cardinal 8335 8337 ),\n",
      "cardinal 8344 8346 (\n",
      "cardinal 8347 8349 )\n",
      "cardinal 8348 8350 ),\n",
      "cardinal 8360 8362 (\n",
      "cardinal 8363 8365 )\n",
      "cardinal 8364 8366 ),\n",
      "cardinal 8370 8372 : (\n",
      "cardinal 8372 8374 i)\n",
      "cardinal 8424 8426 ; (\n",
      "cardinal 8692 8694 c)\n",
      "cardinal 8772 8774 d)\n",
      "cardinal 8793 8795 e)\n",
      "cardinal 8815 8817 i)\n",
      "cardinal 8849 8851 ),\n",
      "cardinal 8864 8866 \n",
      "(\n",
      "cardinal 8884 8886 \n",
      "(\n",
      "cardinal 8911 8913 f)\n",
      "cardinal 8959 8961 a)\n",
      "cardinal 8963 8965 f)\n",
      "cardinal 9009 9011 2.\n",
      "cardinal 9009 9011 2.\n",
      "cardinal 9156 9158 V)\n",
      "cardinal 9288 9290 3.\n",
      "cardinal 9288 9290 3.\n",
      "cardinal 9298 9300 a)\n",
      "cardinal 9390 9392 \n",
      "(\n",
      "cardinal 9392 9394 b)\n",
      "cardinal 9489 9491 4.\n",
      "cardinal 9489 9491 4.\n",
      "cardinal 9502 9504 \n",
      "(\n",
      "cardinal 9504 9506 a)\n",
      "cardinal 9539 9541 \n",
      "(\n",
      "cardinal 9541 9543 b)\n",
      "cardinal 9554 9556 \n",
      "(\n",
      "cardinal 9556 9558 i)\n",
      "cardinal 9569 9571 (\n",
      "cardinal 9573 9575 )\n",
      "cardinal 9577 9579 \n",
      "(\n",
      "cardinal 9596 9598 c)\n",
      "cardinal 9625 9627 (\n",
      "cardinal 9629 9631 )\n",
      "cardinal 9636 9638 \n",
      "(\n",
      "cardinal 9638 9640 d)\n",
      "cardinal 9679 9681 (\n",
      "cardinal 9685 9687 )\n",
      "cardinal 9686 9688 );\n",
      "cardinal 9689 9691 \n",
      "(\n",
      "cardinal 9691 9693 e)\n",
      "cardinal 9711 9713 \n",
      "(\n",
      "cardinal 9713 9715 f)\n",
      "cardinal 9780 9782 \n",
      "(\n",
      "cardinal 9782 9784 g)\n",
      "cardinal 9823 9825 a)\n",
      "cardinal 9827 9829 f)\n",
      "cardinal 9828 9830 ).\n",
      "cardinal 9831 9833 5.\n",
      "cardinal 9846 9848 \n",
      "(\n",
      "cardinal 9848 9850 a)\n",
      "cardinal 9868 9870 b)\n",
      "cardinal 9899 9901 c)\n",
      "cardinal 10038 10040 d)\n",
      "cardinal 10137 10139 6.\n",
      "cardinal 10137 10139 6.\n",
      "cardinal 10154 10156 \n",
      "(\n",
      "cardinal 10156 10158 a)\n",
      "cardinal 10196 10198 )\n",
      "cardinal 10200 10202 \n",
      "(\n",
      "cardinal 10202 10204 i)\n",
      "cardinal 10239 10241 \n",
      "(\n",
      "cardinal 10274 10276 \n",
      "(\n",
      "cardinal 10320 10322 \n",
      "\n",
      "cardinal 10337 10339 ).\n",
      "cardinal 10343 10345 S\n",
      "cardinal 10356 10358 \n",
      "\n",
      "cardinal 10373 10375 ,\n",
      "cardinal 10374 10376 , \n",
      "cardinal 10410 10412 \n",
      "(\n",
      "cardinal 10435 10437 b)\n",
      "cardinal 10452 10454 (\n",
      "cardinal 10456 10458 )\n",
      "cardinal 10490 10492 c)\n",
      "cardinal 10519 10521 (\n",
      "cardinal 10522 10524 )\n",
      "cardinal 10523 10525 ) (\n",
      "cardinal 10553 10555 d)\n",
      "cardinal 10596 10598 7.\n",
      "cardinal 10596 10598 7.\n",
      "cardinal 10600 10602 \n",
      "(\n",
      "cardinal 10602 10604 a)\n",
      "cardinal 10604 10606 \t(\n",
      "cardinal 10606 10608 i)\n",
      "cardinal 10614 10616 7,\n",
      "cardinal 10614 10616 7,\n",
      "cardinal 10636 10638 \n",
      "(\n",
      "cardinal 10660 10662 . (\n",
      "cardinal 10666 10668 ).\n",
      "cardinal 10670 10672 b)\n",
      "cardinal 10675 10677 \n",
      "(\n",
      "cardinal 10677 10679 i)\n",
      "cardinal 10714 10716 (\n",
      "cardinal 10718 10720 )\n",
      "cardinal 10734 10736 \n",
      "(\n",
      "cardinal 10754 10756 \n",
      "(\n",
      "cardinal 10756 10758 1)\n",
      "cardinal 10776 10778 \n",
      "(\n",
      "cardinal 10778 10780 2)\n",
      "cardinal 10778 10780 2)\n",
      "cardinal 10800 10802 \n",
      "(\n",
      "cardinal 10802 10804 3)\n",
      "cardinal 10802 10804 3)\n",
      "cardinal 10818 10820 \n",
      "(\n",
      "cardinal 10844 10846 \n",
      "(\n",
      "cardinal 10865 10867 c)\n",
      "cardinal 10900 10902 \n",
      "(\n",
      "cardinal 10902 10904 d)\n",
      "cardinal 10911 10913 \n",
      "(\n",
      "cardinal 10913 10915 i)\n",
      "cardinal 10942 10944 \n",
      "(\n",
      "cardinal 10968 10970 \n",
      "(\n",
      "cardinal 10992 10994 \n",
      "(\n",
      "cardinal 11040 11042 8.\n",
      "cardinal 11040 11042 8.\n",
      "cardinal 11081 11083 9.\n",
      "cardinal 11081 11083 9.\n",
      "cardinal 11197 11199 (\n",
      "cardinal 11200 11202 )\n",
      "cardinal 11201 11203 ),\n",
      "cardinal 11458 11460 1.\n",
      "cardinal 11458 11460 1.\n",
      "cardinal 11491 11493 , \n",
      "cardinal 11501 11503 a \n",
      "cardinal 11504 11506 )\n",
      "cardinal 11505 11507 ).\n",
      "cardinal 11566 11568 2.\n",
      "cardinal 11566 11568 2.\n",
      "cardinal 11751 11753 3.\n",
      "cardinal 11751 11753 3.\n",
      "cardinal 11876 11878 4.\n",
      "cardinal 11876 11878 4.\n",
      "cardinal 11895 11897 i)\n",
      "cardinal 11909 11911 , (\n",
      "cardinal 12002 12004 (\n",
      "cardinal 12007 12009 )\n",
      "cardinal 12019 12021 (\n",
      "cardinal 12023 12025 )\n",
      "cardinal 12024 12026 ).\n",
      "cardinal 12081 12083 (\n",
      "cardinal 12087 12089 )\n",
      "cardinal 12118 12120 , \n",
      "cardinal 12123 12125 )\n",
      "cardinal 12124 12126 ),\n",
      "cardinal 12161 12163 1.\n",
      "cardinal 12211 12213 2.\n",
      "cardinal 12270 12272 3.\n",
      "cardinal 12270 12272 3.\n",
      "cardinal 12342 12344 ),\n",
      "cardinal 12358 12360 4.\n",
      "cardinal 12358 12360 4.\n",
      "cardinal 12392 12394 5.\n",
      "cardinal 12392 12394 5.\n",
      "cardinal 12426 12428 6.\n",
      "cardinal 12426 12428 6.\n",
      "cardinal 12460 12462 a \n",
      "cardinal 12469 12471 (\n",
      "cardinal 12472 12474 )\n",
      "cardinal 12535 12537 )\n",
      "cardinal 12536 12538 ),\n",
      "cardinal 12546 12548 ) (\n",
      "cardinal 12548 12550 a \"\n",
      "cardinal 12552 12554 \")\n",
      "cardinal 12630 12632 , \n",
      "cardinal 12635 12637 )\n",
      "cardinal 12666 12668 , \n",
      "cardinal 12670 12672 )\n",
      "cardinal 12671 12673 ).\n",
      "cardinal 12699 12701 1.\n",
      "cardinal 12699 12701 1.\n",
      "cardinal 12806 12808 2.\n",
      "cardinal 12806 12808 2.\n",
      "cardinal 12818 12820 (\n",
      "cardinal 12822 12824 )\n",
      "cardinal 12855 12857 2017/589.\n",
      "cardinal 13008 13010 3.\n",
      "cardinal 13070 13072 4.\n",
      "cardinal 13070 13072 4.\n",
      "cardinal 13195 13197 5.\n",
      "cardinal 13195 13197 5.\n",
      "cardinal 13276 13278 \n",
      "_\n",
      "cardinal 13277 13279 __\n",
      "cardinal 13278 13280 __\n",
      "cardinal 13279 13281 __\n",
      "cardinal 13280 13282 __\n",
      "cardinal 13281 13283 __\n",
      "cardinal 13282 13284 __\n",
      "cardinal 13283 13285 __\n",
      "cardinal 13284 13286 __\n",
      "cardinal 13285 13287 __\n",
      "cardinal 13286 13288 __\n",
      "cardinal 13287 13289 __\n",
      "cardinal 13288 13290 __\n",
      "cardinal 13289 13291 __\n",
      "cardinal 13290 13292 __\n",
      "cardinal 13291 13293 __\n",
      "cardinal 13292 13294 __\n",
      "cardinal 13293 13295 __\n",
      "cardinal 13294 13296 __\n",
      "cardinal 13295 13297 __\n",
      "cardinal 13296 13298 __\n",
      "cardinal 13297 13299 __\n",
      "cardinal 13298 13300 __\n",
      "cardinal 13299 13301 __\n",
      "cardinal 13300 13302 __\n",
      "cardinal 13301 13303 __\n",
      "cardinal 13302 13304 __\n",
      "cardinal 13303 13305 __\n",
      "cardinal 13304 13306 __\n",
      "cardinal 13305 13307 __\n",
      "cardinal 13306 13308 __\n",
      "cardinal 13307 13309 __\n",
      "cardinal 13308 13310 __\n",
      "cardinal 13309 13311 __\n",
      "cardinal 13310 13312 __\n",
      "cardinal 13311 13313 __\n",
      "cardinal 13313 13315 \n",
      "_\n",
      "cardinal 13314 13316 __\n",
      "cardinal 13315 13317 __\n",
      "cardinal 13316 13318 __\n",
      "cardinal 13317 13319 __\n",
      "cardinal 13318 13320 __\n",
      "cardinal 13319 13321 __\n",
      "cardinal 13320 13322 __\n",
      "cardinal 13321 13323 __\n",
      "cardinal 13322 13324 __\n",
      "cardinal 13323 13325 __\n",
      "cardinal 13324 13326 __\n",
      "cardinal 13325 13327 __\n",
      "cardinal 13326 13328 __\n",
      "cardinal 13327 13329 __\n",
      "cardinal 13328 13330 __\n",
      "cardinal 13329 13331 __\n",
      "cardinal 13330 13332 __\n",
      "cardinal 13331 13333 __\n",
      "cardinal 13332 13334 __\n",
      "cardinal 13333 13335 __\n",
      "cardinal 13334 13336 __\n",
      "cardinal 13335 13337 __\n",
      "cardinal 13336 13338 __\n",
      "cardinal 13337 13339 __\n",
      "cardinal 13338 13340 __\n",
      "cardinal 13339 13341 __\n",
      "cardinal 13340 13342 __\n",
      "cardinal 13341 13343 __\n",
      "cardinal 13342 13344 __\n",
      "cardinal 13343 13345 __\n",
      "cardinal 13344 13346 __\n",
      "cardinal 13345 13347 __\n",
      "cardinal 13346 13348 __\n",
      "cardinal 13347 13349 __\n",
      "cardinal 13348 13350 __\n",
      "cardinal 13350 13352 \n",
      "_\n",
      "cardinal 13351 13353 __\n",
      "cardinal 13352 13354 __\n",
      "cardinal 13353 13355 __\n",
      "cardinal 13354 13356 __\n",
      "cardinal 13355 13357 __\n",
      "cardinal 13356 13358 __\n",
      "cardinal 13357 13359 __\n",
      "cardinal 13358 13360 __\n",
      "cardinal 13359 13361 __\n",
      "cardinal 13360 13362 __\n",
      "cardinal 13361 13363 __\n",
      "cardinal 13362 13364 __\n",
      "cardinal 13363 13365 __\n",
      "cardinal 13364 13366 __\n",
      "cardinal 13365 13367 __\n",
      "cardinal 13366 13368 __\n",
      "cardinal 13367 13369 __\n",
      "cardinal 13368 13370 __\n",
      "cardinal 13369 13371 __\n",
      "cardinal 13370 13372 __\n",
      "cardinal 13371 13373 __\n",
      "cardinal 13372 13374 __\n",
      "cardinal 13373 13375 __\n",
      "cardinal 13374 13376 __\n",
      "cardinal 13375 13377 __\n",
      "cardinal 13376 13378 __\n",
      "cardinal 13377 13379 __\n",
      "cardinal 13378 13380 __\n",
      "cardinal 13379 13381 __\n",
      "cardinal 13380 13382 __\n",
      "cardinal 13381 13383 __\n",
      "cardinal 13382 13384 __\n",
      "cardinal 13383 13385 __\n",
      "cardinal 13384 13386 __\n",
      "cardinal 13385 13387 __\n",
      "cardinal 13390 13392  _\n",
      "cardinal 13391 13393 __\n",
      "cardinal 13392 13394 __\n",
      "cardinal 13393 13395 __\n",
      "cardinal 13394 13396 __\n",
      "cardinal 13395 13397 __\n",
      "cardinal 13396 13398 __\n",
      "cardinal 13397 13399 __\n",
      "cardinal 13398 13400 __\n",
      "cardinal 13399 13401 __\n",
      "cardinal 13400 13402 __\n",
      "cardinal 13401 13403 __\n",
      "cardinal 13402 13404 __\n",
      "cardinal 13403 13405 __\n",
      "cardinal 13404 13406 __\n",
      "cardinal 13405 13407 __\n",
      "cardinal 13406 13408 __\n",
      "cardinal 13407 13409 __\n",
      "cardinal 13408 13410 __\n",
      "cardinal 13409 13411 __\n",
      "cardinal 13410 13412 __\n",
      "cardinal 13411 13413 __\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardinal 13412 13414 __\n",
      "cardinal 13413 13415 __\n",
      "cardinal 13414 13416 __\n",
      "cardinal 13415 13417 __\n",
      "cardinal 13416 13418 __\n",
      "cardinal 13463 13465 ):\n",
      "cardinal 13465 13467 \n",
      "_\n",
      "cardinal 13466 13468 __\n",
      "cardinal 13467 13469 __\n",
      "cardinal 13468 13470 __\n",
      "cardinal 13469 13471 __\n",
      "cardinal 13470 13472 __\n",
      "cardinal 13471 13473 __\n",
      "cardinal 13472 13474 __\n",
      "cardinal 13473 13475 __\n",
      "cardinal 13474 13476 __\n",
      "cardinal 13475 13477 __\n",
      "cardinal 13476 13478 __\n",
      "cardinal 13477 13479 __\n",
      "cardinal 13478 13480 __\n",
      "cardinal 13479 13481 __\n",
      "cardinal 13480 13482 __\n",
      "cardinal 13481 13483 __\n",
      "cardinal 13482 13484 __\n",
      "cardinal 13483 13485 __\n",
      "cardinal 13484 13486 __\n",
      "cardinal 13485 13487 __\n",
      "cardinal 13486 13488 __\n",
      "cardinal 13487 13489 __\n",
      "cardinal 13488 13490 __\n",
      "cardinal 13489 13491 __\n",
      "cardinal 13490 13492 __\n",
      "cardinal 13491 13493 __\n",
      "cardinal 13492 13494 __\n",
      "cardinal 13493 13495 __\n",
      "cardinal 13494 13496 __\n",
      "cardinal 13495 13497 __\n",
      "cardinal 13496 13498 __\n",
      "cardinal 13497 13499 __\n",
      "cardinal 13498 13500 __\n",
      "cardinal 13499 13501 __\n",
      "cardinal 13500 13502 __\n",
      "cardinal 13502 13504 \n",
      "_\n",
      "cardinal 13503 13505 __\n",
      "cardinal 13504 13506 __\n",
      "cardinal 13505 13507 __\n",
      "cardinal 13506 13508 __\n",
      "cardinal 13507 13509 __\n",
      "cardinal 13508 13510 __\n",
      "cardinal 13509 13511 __\n",
      "cardinal 13510 13512 __\n",
      "cardinal 13511 13513 __\n",
      "cardinal 13512 13514 __\n",
      "cardinal 13513 13515 __\n",
      "cardinal 13514 13516 __\n",
      "cardinal 13515 13517 __\n",
      "cardinal 13516 13518 __\n",
      "cardinal 13517 13519 __\n",
      "cardinal 13518 13520 __\n",
      "cardinal 13519 13521 __\n",
      "cardinal 13520 13522 __\n",
      "cardinal 13521 13523 __\n",
      "cardinal 13522 13524 __\n",
      "cardinal 13523 13525 __\n",
      "cardinal 13524 13526 __\n",
      "cardinal 13525 13527 __\n",
      "cardinal 13526 13528 __\n",
      "cardinal 13527 13529 __\n",
      "cardinal 13528 13530 __\n",
      "cardinal 13529 13531 __\n",
      "cardinal 13530 13532 __\n",
      "cardinal 13531 13533 __\n",
      "cardinal 13532 13534 __\n",
      "cardinal 13533 13535 __\n",
      "cardinal 13534 13536 __\n",
      "cardinal 13535 13537 __\n",
      "cardinal 13536 13538 __\n",
      "cardinal 13537 13539 __\n",
      "cardinal 13539 13541 \n",
      "_\n",
      "cardinal 13540 13542 __\n",
      "cardinal 13541 13543 __\n",
      "cardinal 13542 13544 __\n",
      "cardinal 13543 13545 __\n",
      "cardinal 13544 13546 __\n",
      "cardinal 13545 13547 __\n",
      "cardinal 13546 13548 __\n",
      "cardinal 13547 13549 __\n",
      "cardinal 13548 13550 __\n",
      "cardinal 13549 13551 __\n",
      "cardinal 13550 13552 __\n",
      "cardinal 13551 13553 __\n",
      "cardinal 13552 13554 __\n",
      "cardinal 13553 13555 __\n",
      "cardinal 13554 13556 __\n",
      "cardinal 13555 13557 __\n",
      "cardinal 13556 13558 __\n",
      "cardinal 13557 13559 __\n",
      "cardinal 13558 13560 __\n",
      "cardinal 13559 13561 __\n",
      "cardinal 13560 13562 __\n",
      "cardinal 13561 13563 __\n",
      "cardinal 13562 13564 __\n",
      "cardinal 13563 13565 __\n",
      "cardinal 13564 13566 __\n",
      "cardinal 13565 13567 __\n",
      "cardinal 13566 13568 __\n",
      "cardinal 13567 13569 __\n",
      "cardinal 13568 13570 __\n",
      "cardinal 13569 13571 __\n",
      "cardinal 13570 13572 __\n",
      "cardinal 13571 13573 __\n",
      "cardinal 13572 13574 __\n",
      "cardinal 13573 13575 __\n",
      "cardinal 13574 13576 __\n",
      "cardinal 13579 13581  _\n",
      "cardinal 13580 13582 __\n",
      "cardinal 13581 13583 __\n",
      "cardinal 13582 13584 __\n",
      "cardinal 13583 13585 __\n",
      "cardinal 13584 13586 __\n",
      "cardinal 13585 13587 __\n",
      "cardinal 13586 13588 __\n",
      "cardinal 13587 13589 __\n",
      "cardinal 13588 13590 __\n",
      "cardinal 13589 13591 __\n",
      "cardinal 13590 13592 __\n",
      "cardinal 13591 13593 __\n",
      "cardinal 13592 13594 __\n",
      "cardinal 13593 13595 __\n",
      "cardinal 13594 13596 __\n",
      "cardinal 13595 13597 __\n",
      "cardinal 13596 13598 __\n",
      "cardinal 13597 13599 __\n",
      "cardinal 13598 13600 __\n",
      "cardinal 13599 13601 __\n",
      "cardinal 13600 13602 __\n",
      "cardinal 13601 13603 __\n",
      "cardinal 13602 13604 __\n",
      "cardinal 13603 13605 __\n",
      "cardinal 13604 13606 __\n",
      "cardinal 13605 13607 __\n",
      "cardinal 13647 13649 \n",
      "_\n",
      "cardinal 13648 13650 __\n",
      "cardinal 13649 13651 __\n",
      "cardinal 13650 13652 __\n",
      "cardinal 13651 13653 __\n",
      "cardinal 13652 13654 __\n",
      "cardinal 13653 13655 __\n",
      "cardinal 13654 13656 __\n",
      "cardinal 13655 13657 __\n",
      "cardinal 13656 13658 __\n",
      "cardinal 13657 13659 __\n",
      "cardinal 13658 13660 __\n",
      "cardinal 13659 13661 __\n",
      "cardinal 13660 13662 __\n",
      "cardinal 13661 13663 __\n",
      "cardinal 13662 13664 __\n",
      "cardinal 13663 13665 __\n",
      "cardinal 13664 13666 __\n",
      "cardinal 13665 13667 __\n",
      "cardinal 13666 13668 __\n",
      "cardinal 13667 13669 __\n",
      "cardinal 13668 13670 __\n",
      "cardinal 13669 13671 __\n",
      "cardinal 13670 13672 __\n",
      "cardinal 13671 13673 __\n",
      "cardinal 13672 13674 __\n",
      "cardinal 13673 13675 __\n",
      "cardinal 13674 13676 __\n",
      "cardinal 13675 13677 __\n",
      "cardinal 13676 13678 __\n",
      "cardinal 13677 13679 __\n",
      "cardinal 13678 13680 __\n",
      "cardinal 13679 13681 __\n",
      "cardinal 13680 13682 __\n",
      "cardinal 13681 13683 __\n",
      "cardinal 13682 13684 __\n",
      "cardinal 13684 13686 \n",
      "_\n",
      "cardinal 13685 13687 __\n",
      "cardinal 13686 13688 __\n",
      "cardinal 13687 13689 __\n",
      "cardinal 13688 13690 __\n",
      "cardinal 13689 13691 __\n",
      "cardinal 13690 13692 __\n",
      "cardinal 13691 13693 __\n",
      "cardinal 13692 13694 __\n",
      "cardinal 13693 13695 __\n",
      "cardinal 13694 13696 __\n",
      "cardinal 13695 13697 __\n",
      "cardinal 13696 13698 __\n",
      "cardinal 13697 13699 __\n",
      "cardinal 13698 13700 __\n",
      "cardinal 13699 13701 __\n",
      "cardinal 13700 13702 __\n",
      "cardinal 13701 13703 __\n",
      "cardinal 13702 13704 __\n",
      "cardinal 13703 13705 __\n",
      "cardinal 13704 13706 __\n",
      "cardinal 13705 13707 __\n",
      "cardinal 13706 13708 __\n",
      "cardinal 13707 13709 __\n",
      "cardinal 13708 13710 __\n",
      "cardinal 13709 13711 __\n",
      "cardinal 13710 13712 __\n",
      "cardinal 13711 13713 __\n",
      "cardinal 13712 13714 __\n",
      "cardinal 13713 13715 __\n",
      "cardinal 13714 13716 __\n",
      "cardinal 13715 13717 __\n",
      "cardinal 13716 13718 __\n",
      "cardinal 13717 13719 __\n",
      "cardinal 13718 13720 __\n",
      "cardinal 13719 13721 __\n",
      "cardinal 13721 13723 \n",
      "_\n",
      "cardinal 13722 13724 __\n",
      "cardinal 13723 13725 __\n",
      "cardinal 13724 13726 __\n",
      "cardinal 13725 13727 __\n",
      "cardinal 13726 13728 __\n",
      "cardinal 13727 13729 __\n",
      "cardinal 13728 13730 __\n",
      "cardinal 13729 13731 __\n",
      "cardinal 13730 13732 __\n",
      "cardinal 13731 13733 __\n",
      "cardinal 13732 13734 __\n",
      "cardinal 13733 13735 __\n",
      "cardinal 13734 13736 __\n",
      "cardinal 13735 13737 __\n",
      "cardinal 13736 13738 __\n",
      "cardinal 13737 13739 __\n",
      "cardinal 13738 13740 __\n",
      "cardinal 13739 13741 __\n",
      "cardinal 13740 13742 __\n",
      "cardinal 13741 13743 __\n",
      "cardinal 13742 13744 __\n",
      "cardinal 13743 13745 __\n",
      "cardinal 13744 13746 __\n",
      "cardinal 13745 13747 __\n",
      "cardinal 13746 13748 __\n",
      "cardinal 13747 13749 __\n",
      "cardinal 13748 13750 __\n",
      "cardinal 13749 13751 __\n",
      "cardinal 13750 13752 __\n",
      "cardinal 13751 13753 __\n",
      "cardinal 13752 13754 __\n",
      "cardinal 13753 13755 __\n",
      "cardinal 13754 13756 __\n",
      "cardinal 13755 13757 __\n",
      "cardinal 13756 13758 __\n",
      "cardinal 13761 13763  _\n",
      "cardinal 13762 13764 __\n",
      "cardinal 13763 13765 __\n",
      "cardinal 13764 13766 __\n",
      "cardinal 13765 13767 __\n",
      "cardinal 13766 13768 __\n",
      "cardinal 13767 13769 __\n",
      "cardinal 13768 13770 __\n",
      "cardinal 13769 13771 __\n",
      "cardinal 13770 13772 __\n",
      "cardinal 13771 13773 __\n",
      "cardinal 13772 13774 __\n",
      "cardinal 13773 13775 __\n",
      "cardinal 13774 13776 __\n",
      "cardinal 13775 13777 __\n",
      "cardinal 13776 13778 __\n",
      "cardinal 13777 13779 __\n",
      "cardinal 13778 13780 __\n",
      "cardinal 13779 13781 __\n",
      "cardinal 13780 13782 __\n",
      "cardinal 13781 13783 __\n",
      "cardinal 13782 13784 __\n",
      "cardinal 13783 13785 __\n",
      "cardinal 13784 13786 __\n",
      "cardinal 13785 13787 __\n",
      "cardinal 13786 13788 __\n",
      "cardinal 13787 13789 __\n",
      "cardinal 13822 13824  _\n",
      "cardinal 13823 13825 __\n",
      "cardinal 13824 13826 __\n",
      "cardinal 13825 13827 __\n",
      "cardinal 13826 13828 __\n",
      "cardinal 13827 13829 __\n",
      "cardinal 13828 13830 __\n",
      "cardinal 13829 13831 __\n",
      "cardinal 13830 13832 __\n",
      "cardinal 13831 13833 __\n",
      "cardinal 13832 13834 __\n",
      "cardinal 13833 13835 __\n",
      "cardinal 13834 13836 __\n",
      "cardinal 13835 13837 __\n",
      "cardinal 13836 13838 __\n",
      "cardinal 13837 13839 __\n",
      "cardinal 13838 13840 __\n",
      "cardinal 13839 13841 __\n",
      "cardinal 13840 13842 __\n",
      "cardinal 13841 13843 __\n",
      "cardinal 13842 13844 __\n",
      "cardinal 13843 13845 __\n",
      "cardinal 13844 13846 __\n",
      "cardinal 13845 13847 __\n",
      "cardinal 13846 13848 __\n",
      "cardinal 13847 13849 __\n",
      "cardinal 13848 13850 __\n",
      "cardinal 13849 13851 __\n",
      "cardinal 13850 13852 __\n",
      "cardinal 13851 13853 __\n",
      "cardinal 13852 13854 __\n",
      "cardinal 13853 13855 __\n",
      "cardinal 13854 13856 __\n",
      "cardinal 13855 13857 __\n",
      "cardinal 13856 13858 __\n",
      "cardinal 13857 13859 __\n",
      "cardinal 13858 13860 __\n",
      "cardinal 13861 13863 __\n",
      "cardinal 13862 13864 __\n",
      "cardinal 13863 13865 __\n",
      "cardinal 13864 13866 __\n",
      "cardinal 13865 13867 __\n",
      "cardinal 13866 13868 __\n",
      "cardinal 13867 13869 __\n",
      "cardinal 13868 13870 __\n",
      "cardinal 13869 13871 __\n",
      "cardinal 13870 13872 __\n",
      "cardinal 13871 13873 __\n",
      "cardinal 13872 13874 __\n",
      "cardinal 13873 13875 __\n",
      "cardinal 13874 13876 __\n",
      "cardinal 13875 13877 __\n",
      "cardinal 13876 13878 __\n",
      "cardinal 13877 13879 __\n",
      "cardinal 13878 13880 __\n",
      "cardinal 13879 13881 __\n",
      "cardinal 13880 13882 __\n",
      "cardinal 13881 13883 __\n",
      "cardinal 13882 13884 __\n",
      "cardinal 13883 13885 __\n",
      "cardinal 13884 13886 __\n",
      "cardinal 13885 13887 __\n",
      "cardinal 13886 13888 __\n",
      "cardinal 13887 13889 __\n",
      "cardinal 13888 13890 __\n",
      "cardinal 13889 13891 __\n",
      "cardinal 13890 13892 __\n",
      "cardinal 13891 13893 __\n",
      "cardinal 13892 13894 __\n",
      "cardinal 13893 13895 __\n",
      "cardinal 13894 13896 __\n",
      "cardinal 13895 13897 __\n",
      "cardinal 13896 13898 __\n",
      "cardinal 13901 13903 __\n",
      "cardinal 13902 13904 __\n",
      "cardinal 13903 13905 __\n",
      "cardinal 13904 13906 __\n",
      "cardinal 13905 13907 __\n",
      "cardinal 13906 13908 __\n",
      "cardinal 13907 13909 __\n",
      "cardinal 13908 13910 __\n",
      "cardinal 13909 13911 __\n",
      "cardinal 13910 13912 __\n",
      "cardinal 13911 13913 __\n",
      "cardinal 13912 13914 __\n",
      "cardinal 13913 13915 __\n",
      "cardinal 13914 13916 __\n",
      "cardinal 13915 13917 __\n",
      "cardinal 13916 13918 __\n",
      "cardinal 13917 13919 __\n",
      "cardinal 13918 13920 __\n",
      "cardinal 13919 13921 __\n",
      "cardinal 13920 13922 __\n",
      "cardinal 13921 13923 __\n",
      "cardinal 13922 13924 __\n",
      "cardinal 13923 13925 __\n",
      "cardinal 13924 13926 __\n",
      "cardinal 13925 13927 __\n",
      "cardinal 13926 13928 __\n",
      "cardinal 13927 13929 __\n",
      "cardinal 13928 13930 __\n",
      "cardinal 13929 13931 __\n",
      "cardinal 13930 13932 __\n",
      "cardinal 13931 13933 __\n",
      "cardinal 13932 13934 __\n",
      "cardinal 13933 13935 __\n",
      "cardinal 13934 13936 __\n",
      "cardinal 13935 13937 __\n",
      "cardinal 13936 13938 __\n",
      "cardinal 13941 13943 __\n",
      "cardinal 13942 13944 __\n",
      "cardinal 13943 13945 __\n",
      "cardinal 13944 13946 __\n",
      "cardinal 13945 13947 __\n",
      "cardinal 13946 13948 __\n",
      "cardinal 13947 13949 __\n",
      "cardinal 13948 13950 __\n",
      "cardinal 13949 13951 __\n",
      "cardinal 13950 13952 __\n",
      "cardinal 13951 13953 __\n",
      "cardinal 13952 13954 __\n",
      "cardinal 13953 13955 __\n",
      "cardinal 13954 13956 __\n",
      "cardinal 13955 13957 __\n",
      "cardinal 13956 13958 __\n",
      "cardinal 13957 13959 __\n",
      "cardinal 13958 13960 __\n",
      "cardinal 13959 13961 __\n",
      "cardinal 13960 13962 __\n",
      "cardinal 13961 13963 __\n",
      "cardinal 13962 13964 __\n",
      "cardinal 13963 13965 __\n",
      "cardinal 13964 13966 __\n",
      "cardinal 13965 13967 __\n",
      "cardinal 13966 13968 __\n",
      "cardinal 13967 13969 __\n",
      "cardinal 13968 13970 __\n",
      "cardinal 13969 13971 __\n",
      "cardinal 13970 13972 __\n",
      "cardinal 13971 13973 __\n",
      "cardinal 13972 13974 __\n",
      "cardinal 13973 13975 __\n",
      "cardinal 13974 13976 __\n",
      "cardinal 13975 13977 __\n",
      "cardinal 13976 13978 __\n",
      "cardinal 14015 14017  _\n",
      "cardinal 14016 14018 __\n",
      "cardinal 14017 14019 __\n",
      "cardinal 14018 14020 __\n",
      "cardinal 14019 14021 __\n",
      "cardinal 14020 14022 __\n",
      "cardinal 14021 14023 __\n",
      "cardinal 14022 14024 __\n",
      "cardinal 14023 14025 __\n",
      "cardinal 14024 14026 __\n",
      "cardinal 14025 14027 __\n",
      "cardinal 14026 14028 __\n",
      "cardinal 14027 14029 __\n",
      "cardinal 14028 14030 __\n",
      "cardinal 14029 14031 __\n",
      "cardinal 14030 14032 __\n",
      "cardinal 14031 14033 __\n",
      "cardinal 14032 14034 __\n",
      "cardinal 14033 14035 __\n",
      "cardinal 14034 14036 __\n",
      "cardinal 14035 14037 __\n",
      "cardinal 14036 14038 __\n",
      "cardinal 14037 14039 __\n",
      "cardinal 14038 14040 __\n",
      "cardinal 14039 14041 __\n",
      "cardinal 14040 14042 __\n",
      "cardinal 14041 14043 __\n",
      "cardinal 14042 14044 __\n",
      "cardinal 14043 14045 __\n",
      "cardinal 14044 14046 __\n",
      "cardinal 14045 14047 __\n",
      "cardinal 14046 14048 __\n",
      "cardinal 14047 14049 __\n",
      "cardinal 14048 14050 __\n",
      "cardinal 14049 14051 __\n",
      "cardinal 14050 14052 __\n",
      "cardinal 14051 14053 __\n",
      "cardinal 14054 14056 __\n",
      "cardinal 14055 14057 __\n",
      "cardinal 14056 14058 __\n",
      "cardinal 14057 14059 __\n",
      "cardinal 14058 14060 __\n",
      "cardinal 14059 14061 __\n",
      "cardinal 14060 14062 __\n",
      "cardinal 14061 14063 __\n",
      "cardinal 14062 14064 __\n",
      "cardinal 14063 14065 __\n",
      "cardinal 14064 14066 __\n",
      "cardinal 14065 14067 __\n",
      "cardinal 14066 14068 __\n",
      "cardinal 14067 14069 __\n",
      "cardinal 14068 14070 __\n",
      "cardinal 14069 14071 __\n",
      "cardinal 14070 14072 __\n",
      "cardinal 14071 14073 __\n",
      "cardinal 14072 14074 __\n",
      "cardinal 14073 14075 __\n",
      "cardinal 14074 14076 __\n",
      "cardinal 14075 14077 __\n",
      "cardinal 14076 14078 __\n",
      "cardinal 14077 14079 __\n",
      "cardinal 14078 14080 __\n",
      "cardinal 14079 14081 __\n",
      "cardinal 14080 14082 __\n",
      "cardinal 14081 14083 __\n",
      "cardinal 14082 14084 __\n",
      "cardinal 14083 14085 __\n",
      "cardinal 14084 14086 __\n",
      "cardinal 14085 14087 __\n",
      "cardinal 14086 14088 __\n",
      "cardinal 14087 14089 __\n",
      "cardinal 14088 14090 __\n",
      "cardinal 14089 14091 __\n",
      "cardinal 14094 14096 __\n",
      "cardinal 14095 14097 __\n",
      "cardinal 14096 14098 __\n",
      "cardinal 14097 14099 __\n",
      "cardinal 14098 14100 __\n",
      "cardinal 14099 14101 __\n",
      "cardinal 14100 14102 __\n",
      "cardinal 14101 14103 __\n",
      "cardinal 14102 14104 __\n",
      "cardinal 14103 14105 __\n",
      "cardinal 14104 14106 __\n",
      "cardinal 14105 14107 __\n",
      "cardinal 14106 14108 __\n",
      "cardinal 14107 14109 __\n",
      "cardinal 14108 14110 __\n",
      "cardinal 14109 14111 __\n",
      "cardinal 14110 14112 __\n",
      "cardinal 14111 14113 __\n",
      "cardinal 14112 14114 __\n",
      "cardinal 14113 14115 __\n",
      "cardinal 14114 14116 __\n",
      "cardinal 14115 14117 __\n",
      "cardinal 14116 14118 __\n",
      "cardinal 14117 14119 __\n",
      "cardinal 14118 14120 __\n",
      "cardinal 14119 14121 __\n",
      "cardinal 14120 14122 __\n",
      "cardinal 14121 14123 __\n",
      "cardinal 14122 14124 __\n",
      "cardinal 14123 14125 __\n",
      "cardinal 14124 14126 __\n",
      "cardinal 14125 14127 __\n",
      "cardinal 14126 14128 __\n",
      "cardinal 14127 14129 __\n",
      "cardinal 14128 14130 __\n",
      "cardinal 14129 14131 __\n",
      "cardinal 14134 14136 __\n",
      "cardinal 14135 14137 __\n",
      "cardinal 14136 14138 __\n",
      "cardinal 14137 14139 __\n",
      "cardinal 14138 14140 __\n",
      "cardinal 14139 14141 __\n",
      "cardinal 14140 14142 __\n",
      "cardinal 14141 14143 __\n",
      "cardinal 14142 14144 __\n",
      "cardinal 14143 14145 __\n",
      "cardinal 14144 14146 __\n",
      "cardinal 14145 14147 __\n",
      "cardinal 14146 14148 __\n",
      "cardinal 14147 14149 __\n",
      "cardinal 14148 14150 __\n",
      "cardinal 14149 14151 __\n",
      "cardinal 14150 14152 __\n",
      "cardinal 14151 14153 __\n",
      "cardinal 14152 14154 __\n",
      "cardinal 14153 14155 __\n",
      "cardinal 14154 14156 __\n",
      "cardinal 14155 14157 __\n",
      "cardinal 14156 14158 __\n",
      "cardinal 14157 14159 __\n",
      "cardinal 14158 14160 __\n",
      "cardinal 14159 14161 __\n",
      "cardinal 14160 14162 __\n",
      "cardinal 14161 14163 __\n",
      "cardinal 14162 14164 __\n",
      "cardinal 14163 14165 __\n",
      "cardinal 14164 14166 __\n",
      "cardinal 14165 14167 __\n",
      "cardinal 14166 14168 __\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardinal 14167 14169 __\n",
      "cardinal 14168 14170 __\n",
      "cardinal 14169 14171 __\n"
     ]
    }
   ],
   "source": [
    "#spacy_nlp\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# add match ID \"HelloWorld\" with no callback and one pattern\n",
    "\n",
    "matcher.add('cardinal', None, [{'ENT_TYPE': 'CARDINAL'}, {'IS_PUNCT': True}],[{'LENGTH': 1}, {'IS_PUNCT': True}] )\n",
    "\n",
    "file_text_lines = file_text.splitlines()\n",
    "for line in [file_text]:#file_text_lines[3:10]:\n",
    "    #print (line)\n",
    "    if(line != ''):\n",
    "        doc = spacy_nlp(line)\n",
    "        matches = matcher(doc)\n",
    "        #print (len(list(doc.sents)))\n",
    "        for match_id, start, end in matches:\n",
    "            string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "            span = doc[start:end]  # the matched span\n",
    "            \n",
    "            print( start, end, span.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Structured Annotations of Queries\n",
      "\n",
      "Rifat Reza Joyee\n",
      "joyee@stanford.edu\n",
      "\n",
      "Panagiotis Papadimitriou\n",
      "ppapadim@stanford.edu\n",
      "\n",
      "ABSTRACT\n",
      "We deal with the problem of semantic annotation of free-text\n",
      "queries over a collection of structured documents. In par-\n",
      "ticular, we study structured documents that describe food\n",
      "items and contain attributes such as name, brand, category,\n",
      "ingredients, nutrients, etc. Example of user queries arehigh\n",
      "protein chocolate bar or no sugar coke. The problem is\n",
      "to map the query parts to attributes of the structured docu-\n",
      "ments. For example, in the first query high protein refers\n",
      "to the nutrients attribute and chocolate bar refers to the\n",
      "category. In the second query no sugar refers to ingre-\n",
      "dients and coke refers either to the name or the brand\n",
      "attribute. The challenges in this problem are that the user\n",
      "queries are short and they are less structured compared to\n",
      "regular sentences.There is no training set consisting of an-\n",
      "notated queries (it is expensive to obtain such a set). We\n",
      "thus propose a model combining language model and max-\n",
      "imum entroy classifier for scoring mechanism, for assessing\n",
      "the likelihood of a structured annotation. Our techniques\n",
      "are completely unsupervised, obviating the need for costly\n",
      "manual labeling effort.We evaluated our techniques using\n",
      "real world queries and data and present promising experi-\n",
      "mental results.\n",
      "\n",
      "Categories and Subject Descriptors\n",
      "H.4 [Information Systems Applications]: Miscellaneous\n",
      "\n",
      "1. INTRODUCTION\n",
      "The problem we solved is to allow free text queries over a\n",
      "relational table. When people input a search query in shop-\n",
      "ping website, food websites,search engines about product\n",
      "search instead of finding the contextual pages they look for\n",
      "finding asnwer to the particular type of product they have\n",
      "in their mind, and accroding to them the query best de-\n",
      "scribes the problem for retrieving the product they are look-\n",
      "ing for.So,these product searches are evolving from textual\n",
      "information retrieval systems to highly sophisticated answer-\n",
      "ing ecosystems utilizing information from multiple struc-\n",
      "\n",
      "tured data sources. Structured data is usually abstracted as\n",
      "relational tables or XML files, and readily available in pub-\n",
      "licly accessible data repositories after search. Driving the\n",
      "product search evolution are the user needs. With increas-\n",
      "ing frequency users issue queries that target information that\n",
      "does queries about products (e.g., low calroie protein bar,\n",
      "low sugar oat meal, reduced fat blueberry muffins),movie\n",
      "showtime listings (e.g., black swan near stanford), airlines\n",
      "schedules (e.g., flights from san francisco to new york), are\n",
      "only a few examples of queries that are better served us-\n",
      "ing the latent meaning of the query from structured data,\n",
      "rather than a direct structured database query.Even though\n",
      "its clear that using the underlying relation of these queries\n",
      "will give better output but so far these area possess some\n",
      "challenges and thats what motivated our approach of using\n",
      "structed data to annote queries unsupervised.\n",
      "\n",
      "Lets look at a query like no sugar coke. If it was left\n",
      "to relational database query it will return any coke that has\n",
      "sugar as its ingredient, not necessarily diet coke or coke zero.\n",
      "But its pretty apparent the user is looking for a coke that\n",
      "has no sugar. To consider the language context in the query\n",
      "without any acutal supervision is not a trivial problem and\n",
      "this is the probelm we are proposing to solve.\n",
      "\n",
      "The rest of the paper is organized as follows: In Section 6\n",
      "we discuss the related work.\n",
      "\n",
      "2. OUR APPROACH\n",
      "In this paper, we exploit latent structured semantics in prod-\n",
      "uct queries to create mappings to structured data tables and\n",
      "attributes. We call such mappings Structured Annotations.\n",
      "For example an annotation for the query low calorie pro-\n",
      "tein bar specifies the Table = food and the attributes\n",
      "Nutrients=low calorie, ProductName = protein bar. In\n",
      "producing annotations, we assume that all the structured\n",
      "data are given to us in the form of tables. We exploit that\n",
      "to construct a language model that summarizes all the ta-\n",
      "ble and attributes values and utilize it to deterministically\n",
      "produce all possible annotations efficiently. However, gener-\n",
      "ating all possible annotations is not sufficient. We need to\n",
      "estimate the plausibility of each annotation and determine\n",
      "the one that most likely captures the intent of the user.\n",
      "Furthermore, we need to account for the fact that the users\n",
      "do not adhere to the closed world assumption of the struc-\n",
      "tured data: they use keywords that may not be in the closed\n",
      "structured model, and their queries are likely to target in-\n",
      "formation in the product world they think is relevent.\n",
      "\n",
      "\n",
      "\n",
      "Initially what we do with the query is find segmentation of\n",
      "them. We use language model to train the unigram,bigram\n",
      "and trigram model and using these model return the most\n",
      "plausible segmentation using a dynamic programming model\n",
      "which we will describe shortly.\n",
      "\n",
      "We applied maximum entropy classifier to the segmentation\n",
      "to annote each of the segments. We return the most possi-\n",
      "bile annotation of the k top segmentation using maximum\n",
      "entropy model. In addition, it also computes a score for the\n",
      "possibility of the query targeting information outside the\n",
      "structured data collection. As we use sentence probability\n",
      "in a language model while segmenting,so the free text that\n",
      "doesnt belong to the structured data also gets assigned to\n",
      "some segmentation.\n",
      "\n",
      "After query segmentation we apply standard named entity\n",
      "recognition(NER) method on the segmented data, instead\n",
      "of looking at the whole query we decided just to consider\n",
      "one segment inside the segmentation. For this method the\n",
      "training data is the whol corpus from structured data auto-\n",
      "matically loaded based on their type or name. Here we use\n",
      "no manual annotation and thus saving huge amount of hu-\n",
      "man effort. Note that while doing NER as we only consider\n",
      "segment, the NER of rest of the segments in that particular\n",
      "segmentation is of no relevance to the classifier.\n",
      "\n",
      "2.1 Reason for query segmentation\n",
      "It would be desirable if the structural relationship underly-\n",
      "ing the query words could be automatically identified. Query\n",
      "segmentation is one of the first steps in this direction. It\n",
      "aims to separate query words into segments so that each\n",
      "segment maps to a semantic component, which we refer as\n",
      "a concept. For example, for the query low calorie pro-\n",
      "tien bar, [low calorie] [protien bar] is a good segmentation,\n",
      "while [low] [calorie protein] [bar] and [low] [calorie][protien\n",
      "bar] are not, as segments like [low calorie] and [calorie pro-\n",
      "tein] greatly deviate from the intended meaning of the query.\n",
      "Ideally, each segment should map to exactly one concept.\n",
      "For segments like [low calorie prorein bar], the answer of\n",
      "whether it should be left intact as a compound concept or\n",
      "further segmented into multiple atomic concepts depends on\n",
      "the connection strength of the components (i.e. how strong\n",
      "/ often are low calorie and protein bar associated).This\n",
      "leaves some ambiguity in query segmentation, as we will\n",
      "discuss later. Query segmentation is used here as a step to-\n",
      "wards query annotation where we annote the segmentation\n",
      "rather than the query directly. Query segmentation should\n",
      "also be useful in other query processing tasks such as query\n",
      "rewriting and query log analysis, where one would be able\n",
      "to work on semantic concepts instead of individual words.\n",
      "\n",
      "We use ngram interpolation to combine ngram models and to\n",
      "get the segment probability. And also used the EM method\n",
      "to estimate the weight for the interpolation.\n",
      "\n",
      "2.2 Model and Notation\n",
      "Let query q = w1w2 . . . wn where wi denotes the ith word of\n",
      "the query, we let sik = wiwi+1 . . . wi+k1 denotes a segment\n",
      "of the query with k subsequent words starting with the ith\n",
      "word.\n",
      "\n",
      "Segmentation S is a function S(q) = [si1k1 , si2k1 , . . . , simkm ]\n",
      "\n",
      "that splits query q intom consequtive disjoint segments,i.e.,i1 =\n",
      "1,i2 = i1 + k1,. . . ,im = im1 + km1 and im + km = n\n",
      "\n",
      "R(A1, A2, . . . , An) denotes a relation R with attributes A1,\n",
      "A2, . . . , An and A denotes set of all attributes.\n",
      "\n",
      "We define annotation function f(s)  A that returns the\n",
      "attribute correspoding to segment s\n",
      "\n",
      "2.3 Algorithm for query segmentation\n",
      "For two segmentation S1 and S2 of the same query suppose\n",
      "they differ at only the one segment boundry,i.e.,\n",
      "S1 = si1k1 , si2k1 , . . . , si(l1)k(l1), silkl , si(l+1)k(l+1),\n",
      "si(l+2)k(l+2), . . . , simkm\n",
      "\n",
      "S2 = si1k1 , si2k1 , . . . , si(l1)k(l1), s\n",
      "\n",
      "ilkl\n",
      "\n",
      ", si(l+2)k(l+2),\n",
      ". . . , simkm\n",
      "where s\n",
      "\n",
      "\n",
      "ilkl\n",
      "\n",
      "= silkl , si(l+1)k(l+1) is the concatenation of silkl\n",
      "and si(l+1)k(l+1) That means we will favour segmentations\n",
      "\n",
      "with higher probability of generating the query. So in above\n",
      "the language model will have higher probability for P (S1)\n",
      "\n",
      "than P (S2) if and ony if P (silkl , si(l+1)k(l+1)) > P (s\n",
      "\n",
      "ilkl\n",
      "\n",
      ")\n",
      "\n",
      "This is how the algorithm looks like for segmentation given\n",
      "a query\n",
      "\n",
      "Algorithm 1 Dynamic programming algorithm using lan-\n",
      "guage model for k segmenations\n",
      "\n",
      "Input:query w1w2 . . . wn,language model L,number of seg-\n",
      "mentation k\n",
      "Output:top k segmentations with highest likelihood\n",
      "B[i]:top k segmentations for sub-query w1w2 . . . wi\n",
      "For each segmenation b in B[i],segs denotes the segments and\n",
      "prob denotes the probability of the segment in the language\n",
      "model\n",
      "\n",
      "for all i  [1 . . . n] do\n",
      "s w1w2 . . . wi\n",
      "if l.P rob(s) > 0 then\n",
      "a newSegmentation\n",
      "a.segs s\n",
      "a.prob l.P rob(s)\n",
      "end if\n",
      "for all j  [1 . . . i 1] do\n",
      "for all b  B[j] do\n",
      "s wjw(j + 1) . . . wi\n",
      "if l.P rob(s) > 0 then\n",
      "a newSegmentation\n",
      "a.segs b.segss\n",
      "a.prob b.prob  l.P rob(s)\n",
      "B[i] b[i]a\n",
      "end if\n",
      "end for\n",
      "end for\n",
      "sort B[i] by prob\n",
      "truncate B[i] to size k\n",
      "end for\n",
      "\n",
      "2.4 Maximum Entropy Classifier for Annota-\n",
      "tion\n",
      "\n",
      "The goal of statistical modeling is to construct a model that\n",
      "best accounts for some training data. Starting from a set of\n",
      "\n",
      "\n",
      "\n",
      "data such algorithms can automatically extract a set of rela-\n",
      "tionships inherent in the data, and then combine these rules\n",
      "into a model of the data. Maximum Entropy (ME) model\n",
      "is one of the frameworks suggested for this purpose (Ratna-\n",
      "parkhi, 1997). In other words, given an empirical probability\n",
      "distribution, we need to build a model as close to this dis-\n",
      "tribution as possible. Furthermore, the chosen model being\n",
      "consistent with all the facts should be otherwise as uniform\n",
      "as possible. In order to build such a model, a large num-\n",
      "ber of samples (x1, y1), (x2, y2), . . . , (xN , yN ), where x rep-\n",
      "\n",
      "resents a given context and y aAS an outcome, needs to be\n",
      "collected. Then we can summarize the training sample in\n",
      "terms of its probability distribution p (x, y), defined as a\n",
      "number of times the sample (x, y) occurs in the data divided\n",
      "by the overall number of samples in the data. The goal is\n",
      "to choose an optimal model incorporating a set of statistics\n",
      "from the training data (Berger et al., 1996). To do this a\n",
      "set of features that describes the data in the most efficient\n",
      "way needs to be defined. Once a set of features is chosen, we\n",
      "can construct the corresponding ME model by adding fea-\n",
      "tures as constraints to the model and adjust weights of these\n",
      "features. Intuitively, models with high entropy are more uni-\n",
      "form and therefore they assume less about the world. The\n",
      "maximum entropy model can be interpreted as the model\n",
      "that assumes only the knowledge that is represented by the\n",
      "features derived from the training data, and nothing else.\n",
      "\n",
      "The maximum entropy framework estimates probabilities\n",
      "based on the principle of making as few assumptions as pos-\n",
      "sible, other than the constraints imposed. Such constraints\n",
      "are derived from training data, expressing some relationship\n",
      "between features and outcome. The probability distribution\n",
      "that satisfies the above property is the one with the highest\n",
      "entropy. It is unique, agrees with the maximum-likelihood\n",
      "distribution, and has the exponential form (Della Pietra et\n",
      "al., 1997):\n",
      "\n",
      "p(o|h) = 1\n",
      "Z(h)\n",
      "\n",
      "k\n",
      "j=1\n",
      "\n",
      "\n",
      "fj(h,o)\n",
      "\n",
      "j (1)\n",
      "\n",
      "where o refers to outcome, h the history(or context),Z(h)\n",
      "is a normalization function. Each feature fj(h, o) is a bi-\n",
      "nary function.For example, in predicting if a word belongs\n",
      "to a word class,o is either true or false, and h refers to the\n",
      "surrounding context:\n",
      "\n",
      "fj(h, o) =\n",
      "\n",
      "{\n",
      "1 : if = true, previous word = the\n",
      "0 : otherwise\n",
      "\n",
      "The proposed approach consists of training a Maximum En-\n",
      "tropy classifier for the task in question, using a large auto-\n",
      "matically created food related corpus, as opposed to other\n",
      "systems being trained on expensive manually annotated data.\n",
      "First, from database we tag terms belonging to database and\n",
      "train the Maximum Entropy Classifier with it.\n",
      "\n",
      "The basic improvement comes from applying language model\n",
      "for segmentation that we plug in the name entity recognition\n",
      "algorithm using Max Entropy Classifier, any other classifier\n",
      "could be used at this point, logistic regression, maximum\n",
      "likelihood classifier etc. So,as expected,maximum entropy\n",
      "classifier trained on automatically created data and with\n",
      "previously segmented query performs magically better than\n",
      "the baseline recognizer itself, which proves the efficiency of\n",
      "the adopted approach.\n",
      "\n",
      "Algorithm 2 ComputeAnnotation\n",
      "\n",
      "Input:List of Segmentation\n",
      "Output:AnnotatedSegmentation\n",
      "B[i] is a List of Segmentation each with segments having\n",
      "possible top K segmentation from the query using algorithm\n",
      "1\n",
      "\n",
      "for all b  B[i] do\n",
      "aSs \n",
      "for all seg  b do\n",
      "lP = maxentClassifier.getLP (seg)\n",
      "aS  newAnnotatedSegment(seg, lP [0], lP [1])\n",
      "aSs aSsaS\n",
      "end for\n",
      "end for\n",
      "\n",
      "3. SUPERVISED LEARNING APPROACH\n",
      "This is like classic named entity recognition(NER) problem\n",
      "in natural language processing. This involves query labeling\n",
      "and training with the labelled queries. NER systems have\n",
      "been created that use linguistic grammar-based techniques\n",
      "as well as statistical models. Hand-crafted grammar-based\n",
      "systems typically obtain better precision, but at the cost of\n",
      "lower recall and months of work by experienced computa-\n",
      "tional linguists. Statistical NER systems typically require\n",
      "a large amount of manually annotated training data. Sta-\n",
      "tistical NERs usually find the sequence of tags p(N |S)that\n",
      "maximizes the probability , where S is the sequence of words\n",
      "in a sentence, and N is the sequence of named-entity tags\n",
      "assigned to the word in S\n",
      "\n",
      "Supervised learning methods represents linguistic informa-\n",
      "tion in the form of features. Each feature informs of the\n",
      "occurrence of certain attribute in a context that contains a\n",
      "linguistic ambiguity. That context is the text surrounding\n",
      "this ambiguitiy and relevant to the disambiguation process.\n",
      "The features used can be of distinct nature: word colloca-\n",
      "tions, part-of-speech labels, keywords, topics and domain\n",
      "information, etc. A method using supervised learning tries\n",
      "to classify a context containing an ambiguous word or com-\n",
      "pound word in one of its possible senses by means of a clas-\n",
      "sification function. This function is obtained after a training\n",
      "process on a sense tagged corpus. The information source\n",
      "for this training is the set of results of the features evalua-\n",
      "tion on each context, that is, each context has its vector of\n",
      "feature values. The supervised learning method (Maximum\n",
      "Entropy) used in this paper to do such analysis is based on\n",
      "Maximum Entropy Markov Model (MEMM).\n",
      "\n",
      "3.1 Query Labeling\n",
      "The problem with supervised learning is that a huge corpus\n",
      "has to be annotated manually and using that the proba-\n",
      "bilistic classifier will learn how to guess the annotation of\n",
      "unseen example using viterbi algorithm exploiting MEMM.\n",
      "We used an algorithm to annotate data automatically as if\n",
      "they were manually annotated. We assumed a form of the\n",
      "search query for it. First from the strcutured data we pick\n",
      "a row and then flip a coin to decide whether to use that line\n",
      "for training or not.If its selected the next step is to use ei-\n",
      "ther productName,typeName or brandName. Then we flip\n",
      "a coin again to decide whether to do two more annotation\n",
      "or one. Whether its one or two its randomly chosen from\n",
      "\n",
      "\n",
      "\n",
      "ingredients,nutrients,allergens.\n",
      "\n",
      "4. UNSUPERVISED APPROACH\n",
      "This is the approach that we came up with for named entity\n",
      "recognition in a particular context of structured data. Here\n",
      "the annotation is done automatically instead of manually,\n",
      "where each column represents one named entity and thus an\n",
      "algorithm automatically annotes the entry in database later\n",
      "used for training the probability model. At the same time\n",
      "these touples of named entity are used for language model\n",
      "training as well where we ignore the named tag and consider\n",
      "each of the named entity as a sentence for the purpose of\n",
      "training the language model.\n",
      "\n",
      "The following algorithm deals with how to create annotated\n",
      "named entity from structured data automatically.\n",
      "\n",
      "Algorithm 3 Making annotated training data\n",
      "\n",
      "Input:Structured Data\n",
      "Output:List of Pair of sentence and its annota-\n",
      "tion\n",
      "\n",
      "fieldNames fileheader|columnheader\n",
      "for all rowinstructured data do\n",
      "line row\n",
      "field line splitted to field\n",
      "for all f  field do\n",
      "subfield f splitted to subfield\n",
      "for all s insubfield do\n",
      "words each subfield splitted\n",
      "fieldName fieldNames.get(f.index)\n",
      "sentence (words, fieldName)\n",
      "end for\n",
      "end for\n",
      "end for\n",
      "\n",
      "After the named entity has been annotated we can use this\n",
      "existing data structures first element which is the entity to\n",
      "get the sentences that we will need to train the language\n",
      "model. The algorithm is as follows.\n",
      "\n",
      "Algorithm 4 Making training ngram model for segmenta-\n",
      "tion\n",
      "Input:Annotated training data,n\n",
      "Output:Trained language model\n",
      "\n",
      "sentencess annotatedData.words\n",
      "lmodel  new InterpolationMode(n)\n",
      "lmodel.weight weight found by EM\n",
      "lmodel.train(sentences)\n",
      "\n",
      "4.1 Query Segmentation\n",
      "Use dynamic programming algorithm of [11] to do the seg-\n",
      "mentation using the probabilities that we obtain from the\n",
      "trained language models.However we modified this algorithm\n",
      "in 2.3 to consider the sentence probability using language\n",
      "model instead of calculating the sentence probability using\n",
      "individual probability of words. In this way the free words\n",
      "are taken care of automatically and no additional algorithm\n",
      "is needed to handle the free words. After we ran experient\n",
      "we figured having background knowledge on ngram language\n",
      "\n",
      "model improves the dynamic programic algortihm from its\n",
      "original version.\n",
      "\n",
      "4.2 Segment Classification\n",
      "We use maxEnt classifier to do classification of the already\n",
      "segmented query, that is on the segmentation. The advan-\n",
      "tage here is we donot need to look at the whole query. When\n",
      "estimating the annotation only the segment words are looked\n",
      "at and the context to other segments is not necessary. We\n",
      "use 2.4 to find the segment annotation.\n",
      "\n",
      "5. EXPERIMENTS AND EVALUATION\n",
      "The database has a relation with 30,000 records and 10 at-\n",
      "tributes. We used a test set of 100 manually labelled queries.\n",
      "These data are the structured data of www.caloricious.com.\n",
      "The manually labeled query are from the server logs which\n",
      "stores the actual queries entered by humans. First we ran us-\n",
      "ing the traditional NER which uses MEMM to annotate. We\n",
      "synthetically generated annotated data from corpus which\n",
      "is as good as annotating manually.Lets discuss the struc-\n",
      "tured data before going to the detail of how we generated\n",
      "the synthetical data its logical to discuss the data format in\n",
      "the structured data for caloricious. It has few column heads\n",
      "called product name,type name,brand name,ingredients nu-\n",
      "trients ,allergens. Examples of each of the type are prod-\n",
      "uct name( united bakery salted breadsticks, country white\n",
      "bagels, lemon sorbet), type name( milkshakes, dressings,\n",
      "granola bars), brand name ( hersheys, newtree, ritter sport),\n",
      "ingredients ( sugar, cocoa butter, organic almond butter),\n",
      "nutrients ( vitamin b5, citric acid),allergens( low total fat,\n",
      "low sodium, gluten free). The algorithm discussed in 3.1\n",
      "is used to create annotated training data. Its assumed that\n",
      "users would have either product name, type name or brand\n",
      "name with some of the ingredients, nutrients or allergians,\n",
      "its all randomized to generate differnt combination. We used\n",
      "both this approach and annotating all the existing data row\n",
      "assuming each row to be a long query. The later took a long\n",
      "time to train the MEMM model because of its size however\n",
      "the former was very quick to generate the model as corpus\n",
      "was smaller. However, the later was 53% accurate whereas\n",
      "the former was 49% accurate. This tells that if the corpus is\n",
      "really huge and annotating every datum is not plausible our\n",
      "smart query generator can be used as a reasonable compro-\n",
      "mise for computational compelxity and still generate a good\n",
      "result.Figure 5 shows the comparison between these two.\n",
      "\n",
      "For the unsupervised and our proposed approach we ran on\n",
      "the same data set and there were 6093070 sentences as we\n",
      "made each of the entity in database a sentence. We first\n",
      "tried using unigram model and the accuracy came upto 47%\n",
      "which is just a little(4%) below the MEMM model for an-\n",
      "notated data. Which basically shows the power of language\n",
      "model over annotation. Because even though the accuracy is\n",
      "not higher for unigram, it tells that annotation usually have\n",
      "to be on huge data to make it efficient, because identifying\n",
      "unkown entity gets difficult for supervised learning. In 5\n",
      "we show the NER for different entity and its accuracy com-\n",
      "paring to the orignal and correctly annotated ratio.It does\n",
      "reasonable on ingredients which tells that identifying ingre-\n",
      "dients is easier in general and the feature identification works\n",
      "for ingredients. Next we try to use bigram for segmentation\n",
      "and feed this to our annotation model. Not surprisingly it\n",
      "performs very good and give 82% accuracy. We used interpo-\n",
      "\n",
      "\n",
      "\n",
      "Figure 1: Accuracy using unigram,showing ratio of\n",
      "annotated and real entity\n",
      "\n",
      "Figure 2: Accuracy using bigram,showing ratio of\n",
      "annotated and real entity\n",
      "\n",
      "Figure 3: Accuracy using trigram,showing ratio of\n",
      "annotated and real entity\n",
      "\n",
      "lation for bigram and used EM model to calculate the weight\n",
      "for the model which came up to 0.010394140437218138 for\n",
      "unigram and 0.9896058595627819 for bigram. Fig 5 show\n",
      "the performance of bigram on each of the different named\n",
      "entity.\n",
      "\n",
      "After bigram, we used trigram for the same purpose of doing\n",
      "\n",
      "fig:sup\n",
      "\n",
      "Figure 4: Overall accuracy comparison using uni-\n",
      "gram,bigram,trigram and supervised learning using\n",
      "MEMM\n",
      "\n",
      "Figure 5: Accuracy using smart synthetic data vs\n",
      "annoting the whole corpus\n",
      "\n",
      "query segmentation for feeding it to the annotation system.\n",
      "To our little surprise bigram performed better and trigram\n",
      "gave 79.23% accuracy. For trigram model we used interpo-\n",
      "lation the weight of which is decided using EM algorithm\n",
      "which came up as 0.04033219974762, 0.160914763422446,\n",
      "0.7987530368299335 for unigram,bigram,trigram respectively.\n",
      "What it tells is basically queries are short and the named\n",
      "entity are usually easily identified in two terms or bigram\n",
      "model rather than trigram. So its a good indication why\n",
      "higher model ngram wouldnt be necessary for query segmen-\n",
      "tation.Fig 5 show the performance of bigram on each of the\n",
      "different named entity.\n",
      "\n",
      "So the experiment shows clearly how brininging in languae\n",
      "model dramatically improves the performance of query la-\n",
      "belling. Its a combination of query segmentation and use of\n",
      "language model that made our algorithm to perform really\n",
      "well.\n",
      "\n",
      "6. RELATED WORK\n",
      "We are essentially solving two problems and combining them\n",
      "to solve an interesting problem of query segmentation and\n",
      "annotation. Its combining query segmentaiton and apply\n",
      "annotation on top of it. In natural language processing,\n",
      "\n",
      "\n",
      "\n",
      "there has been a significant amount of research on text seg-\n",
      "mentation, such as noun phrase chunking [[12]], where the\n",
      "task is to recognize the chunks that consist of noun phrases,\n",
      "and Chinese word segmentation [[1]], where the task is to\n",
      "delimit words by putting boundaries between Chinese char-\n",
      "acters. Query segmentation is similar to these problems in\n",
      "the sense that they all try to identify meaningful seman-\n",
      "tic units from the input. However, one may not be able to\n",
      "apply these techniques directly to query segmentation, be-\n",
      "cause Web search query language is very different (queries\n",
      "tend to be short, composed of keywords), and some essential\n",
      "techniques to noun phrase chunking, such as partof- speech\n",
      "tagging [[2]], can not achieve high performance when applied\n",
      "to queries.\n",
      "\n",
      "Interestingly, there is little investigation done of query seg-\n",
      "mentation,let along its use in named entity recognition in\n",
      "spite of its importance. To our knowledge, two approaches\n",
      "have been studied in previous works. The first is based on\n",
      "(pointwise) mutual information (MI) between pairs of query\n",
      "words. If the MI value between two adjacent words is be-\n",
      "low a predefined threshold, a segment boundary is inserted\n",
      "at that position. The problem with this approach is that\n",
      "MI, by definition, cannot capture correlations among more\n",
      "than two words, thus it cannot handle long entities like song\n",
      "names, where MI may be low in certain positions ( e.g., be-\n",
      "\n",
      "tween heart and will in aAIJmy heart will go onaAI).In\n",
      "our approach MI is used between two segments, not two\n",
      "words which is more relevent on the background of languge\n",
      "model.\n",
      "\n",
      "A problem related to generating plausible structured anno-\n",
      "tations, referred to as web query tagging, was introduced\n",
      "in[7] . Its goal is to assign each query term to a specified\n",
      "category, roughly corresponding to a table attribute. A Con-\n",
      "ditional Random Field (CRF) is used to capture dependen-\n",
      "cies between query words and identify the most likely joint\n",
      "assignment of words to aAIJcategoriesaAI.Keyword search\n",
      "on relational [[5], [8], [6]], semi-structured [[3],[9]] and graph\n",
      "data [ [4]] (Keyword Search Over Structured Data, abbre-\n",
      "viated as KSOSD) has been an extremely active research\n",
      "topic. Its goal is the efficient retrieval of relevant database\n",
      "tuples, XML sub-trees or subgraphs in response to keyword\n",
      "queries. The problem is challenging since the relevant pieces\n",
      "of information needed to assemble answers are assumed to be\n",
      "scattered across relational tables, graph nodes, etc. Essen-\n",
      "tially, KSOSD techniques allow users to formulate compli-\n",
      "cated join queries against a database using keywords. The\n",
      "tuples returned are ranked based on the distance in the\n",
      "database of the fragments joined to produce a tuple, and\n",
      "the textual similarity of the fragments to query terms. In\n",
      "[10] the authors study the state of art of the problem we\n",
      "tried to solve but with respect to web queries. It has funda-\n",
      "mental limitations of being very complex and doing EM on\n",
      "the fly all the time which is not feasible for product searches.\n",
      "\n",
      "7. CONCLUSIONS\n",
      "We have proved that query annotation works almost dra-\n",
      "matically better in the new approach we came up with of\n",
      "segmenting the query before and then applying annotaiton\n",
      "having a language model as the base of segmentation. We\n",
      "have used a real exisitng website www.caloricious.com for\n",
      "our experiment and proved our algorithms superiority over\n",
      "\n",
      "the existing algorithm. Our future plan is to run the algo-\n",
      "rithm on other product websites data and see whether the\n",
      "some performance holds and eventually come up with a do-\n",
      "main indpendent model for query segmentation.\n",
      "\n",
      "8. REFERENCES\n",
      "[1] M. R. Brent and T. A. Cartwright. Distributional\n",
      "\n",
      "regularity and phonotactic constraints are useful for\n",
      "segmentation. volume 61, pages 93125, 1996.\n",
      "\n",
      "[2] E. Brill and G. Ngai. Man vs. machine: A case study\n",
      "in base noun phrase learning. 1999.\n",
      "\n",
      "[3] L. Guo, F. Shao, C. Botev, and\n",
      "J. Shanmugasundaram. Xrank: ranked keyword search\n",
      "over xml documents. In Proceedings of the 2003 ACM\n",
      "SIGMOD international conference on Management of\n",
      "data, SIGMOD 03, pages 1627, New York, NY,\n",
      "USA, 2003. ACM.\n",
      "\n",
      "[4] H. He, H. Wang, J. Yang, and P. S. Yu. Blinks:\n",
      "ranked keyword searches on graphs. In Proceedings of\n",
      "the 2007 ACM SIGMOD international conference on\n",
      "Management of data, SIGMOD 07, pages 305316,\n",
      "New York, NY, USA, 2007. ACM.\n",
      "\n",
      "[5] V. Hristidis, L. Gravano, and Y. Papakonstantinou.\n",
      "Efficient ir-style keyword search over relational\n",
      "databases. In Proceedings of the 29th international\n",
      "conference on Very large data bases - Volume 29,\n",
      "VLDB 2003, pages 850861. VLDB Endowment, 2003.\n",
      "\n",
      "[6] E. Kandogan, R. Krishnamurthy, S. Raghavan,\n",
      "S. Vaithyanathan, and H. Zhu. Avatar semantic\n",
      "search: a database approach to information retrieval.\n",
      "In Proceedings of the 2006 ACM SIGMOD\n",
      "international conference on Management of data,\n",
      "SIGMOD 06, pages 790792, New York, NY, USA,\n",
      "2006. ACM.\n",
      "\n",
      "[7] X. Li, Y.-Y. Wang, and A. Acero. Extracting\n",
      "structured information from user queries with\n",
      "semi-supervised conditional random fields. In\n",
      "Proceedings of the 32nd international ACM SIGIR\n",
      "conference on Research and development in\n",
      "information retrieval, SIGIR 09, pages 572579, New\n",
      "York, NY, USA, 2009. ACM.\n",
      "\n",
      "[8] F. Liu, C. Yu, W. Meng, and A. Chowdhury. Effective\n",
      "keyword search in relational databases. In Proceedings\n",
      "of the 2006 ACM SIGMOD international conference\n",
      "on Management of data, SIGMOD 06, pages 563574,\n",
      "New York, NY, USA, 2006. ACM.\n",
      "\n",
      "[9] Z. Liu and Y. Cher. Reasoning and identifying\n",
      "relevant matches for xml keyword search. Proc. VLDB\n",
      "Endow., 1:921932, August 2008.\n",
      "\n",
      "[10] N. Sarkas, S. Paparizos, and P. Tsaparas. Structured\n",
      "annotations of web queries. In Proceedings of the 2010\n",
      "international conference on Management of data,\n",
      "SIGMOD 10, pages 771782, New York, NY, USA,\n",
      "2010. ACM.\n",
      "\n",
      "[11] B. Tan and F. Peng. Unsupervised query segmentation\n",
      "using generative language models and wikipedia. In\n",
      "Proceeding of the 17th international conference on\n",
      "World Wide Web, WWW 08, pages 347356, New\n",
      "York, NY, USA, 2008. ACM.\n",
      "\n",
      "[12] C. Zhang, N. Sun, X. Hu, T. Huang, and T. seng\n",
      "Chua. Query segmentation based on eigenspace\n",
      "similarity.\n",
      "\n",
      "\n",
      "\n",
      "Structured Annotations of Queries\n",
      "\n",
      "Rifat Reza Joyee\n",
      "joyee@stanford.edu\n",
      "\n",
      "Panagiotis Papadimitriou\n",
      "ppapadim@stanford.edu\n",
      "\n",
      "ABSTRACT\n",
      "We deal with the problem of semantic annotation of free-text\n",
      "queries over a collection of structured documents. In par-\n",
      "ticular, we study structured documents that describe food\n",
      "items and contain attributes such as name, brand, category,\n",
      "ingredients, nutrients, etc. Example of user queries arehigh\n",
      "protein chocolate bar or no sugar coke. The problem is\n",
      "to map the query parts to attributes of the structured docu-\n",
      "ments. For example, in the first query high protein refers\n",
      "to the nutrients attribute and chocolate bar refers to the\n",
      "category. In the second query no sugar refers to ingre-\n",
      "dients and coke refers either to the name or the brand\n",
      "attribute. The challenges in this problem are that the user\n",
      "queries are short and they are less structured compared to\n",
      "regular sentences.There is no training set consisting of an-\n",
      "notated queries (it is expensive to obtain such a set). We\n",
      "thus propose a model combining language model and max-\n",
      "imum entroy classifier for scoring mechanism, for assessing\n",
      "the likelihood of a structured annotation. Our techniques\n",
      "are completely unsupervised, obviating the need for costly\n",
      "manual labeling effort.We evaluated our techniques using\n",
      "real world queries and data and present promising experi-\n",
      "mental results.\n",
      "\n",
      "Categories and Subject Descriptors\n",
      "H.4 [Information Systems Applications]: Miscellaneous\n",
      "\n",
      "1. INTRODUCTION\n",
      "The problem we solved is to allow free text queries over a\n",
      "relational table. When people input a search query in shop-\n",
      "ping website, food websites,search engines about product\n",
      "search instead of finding the contextual pages they look for\n",
      "finding asnwer to the particular type of product they have\n",
      "in their mind, and accroding to them the query best de-\n",
      "scribes the problem for retrieving the product they are look-\n",
      "ing for.So,these product searches are evolving from textual\n",
      "information retrieval systems to highly sophisticated answer-\n",
      "ing ecosystems utilizing information from multiple struc-\n",
      "\n",
      "tured data sources. Structured data is usually abstracted as\n",
      "relational tables or XML files, and readily available in pub-\n",
      "licly accessible data repositories after search. Driving the\n",
      "product search evolution are the user needs. With increas-\n",
      "ing frequency users issue queries that target information that\n",
      "does queries about products (e.g., low calroie protein bar,\n",
      "low sugar oat meal, reduced fat blueberry muffins),movie\n",
      "showtime listings (e.g., black swan near stanford), airlines\n",
      "schedules (e.g., flights from san francisco to new york), are\n",
      "only a few examples of queries that are better served us-\n",
      "ing the latent meaning of the query from structured data,\n",
      "rather than a direct structured database query.Even though\n",
      "its clear that using the underlying relation of these queries\n",
      "will give better output but so far these area possess some\n",
      "challenges and thats what motivated our approach of using\n",
      "structed data to annote queries unsupervised.\n",
      "\n",
      "Lets look at a query like no sugar coke. If it was left\n",
      "to relational database query it will return any coke that has\n",
      "sugar as its ingredient, not necessarily diet coke or coke zero.\n",
      "But its pretty apparent the user is looking for a coke that\n",
      "has no sugar. To consider the language context in the query\n",
      "without any acutal supervision is not a trivial problem and\n",
      "this is the probelm we are proposing to solve.\n",
      "\n",
      "The rest of the paper is organized as follows: In Section 6\n",
      "we discuss the related work.\n",
      "\n",
      "2. OUR APPROACH\n",
      "In this paper, we exploit latent structured semantics in prod-\n",
      "uct queries to create mappings to structured data tables and\n",
      "attributes. We call such mappings Structured Annotations.\n",
      "For example an annotation for the query low calorie pro-\n",
      "tein bar specifies the Table = food and the attributes\n",
      "Nutrients=low calorie, ProductName = protein bar. In\n",
      "producing annotations, we assume that all the structured\n",
      "data are given to us in the form of tables. We exploit that\n",
      "to construct a language model that summarizes all the ta-\n",
      "ble and attributes values and utilize it to deterministically\n",
      "produce all possible annotations efficiently. However, gener-\n",
      "ating all possible annotations is not sufficient. We need to\n",
      "estimate the plausibility of each annotation and determine\n",
      "the one that most likely captures the intent of the user.\n",
      "Furthermore, we need to account for the fact that the users\n",
      "do not adhere to the closed world assumption of the struc-\n",
      "tured data: they use keywords that may not be in the closed\n",
      "structured model, and their queries are likely to target in-\n",
      "formation in the product world they think is relevent.\n",
      "\n",
      "\n",
      "\n",
      "Initially what we do with the query is find segmentation of\n",
      "them. We use language model to train the unigram,bigram\n",
      "and trigram model and using these model return the most\n",
      "plausible segmentation using a dynamic programming model\n",
      "which we will describe shortly.\n",
      "\n",
      "We applied maximum entropy classifier to the segmentation\n",
      "to annote each of the segments. We return the most possi-\n",
      "bile annotation of the k top segmentation using maximum\n",
      "entropy model. In addition, it also computes a score for the\n",
      "possibility of the query targeting information outside the\n",
      "structured data collection. As we use sentence probability\n",
      "in a language model while segmenting,so the free text that\n",
      "doesnt belong to the structured data also gets assigned to\n",
      "some segmentation.\n",
      "\n",
      "After query segmentation we apply standard named entity\n",
      "recognition(NER) method on the segmented data, instead\n",
      "of looking at the whole query we decided just to consider\n",
      "one segment inside the segmentation. For this method the\n",
      "training data is the whol corpus from structured data auto-\n",
      "matically loaded based on their type or name. Here we use\n",
      "no manual annotation and thus saving huge amount of hu-\n",
      "man effort. Note that while doing NER as we only consider\n",
      "segment, the NER of rest of the segments in that particular\n",
      "segmentation is of no relevance to the classifier.\n",
      "\n",
      "2.1 Reason for query segmentation\n",
      "It would be desirable if the structural relationship underly-\n",
      "ing the query words could be automatically identified. Query\n",
      "segmentation is one of the first steps in this direction. It\n",
      "aims to separate query words into segments so that each\n",
      "segment maps to a semantic component, which we refer as\n",
      "a concept. For example, for the query low calorie pro-\n",
      "tien bar, [low calorie] [protien bar] is a good segmentation,\n",
      "while [low] [calorie protein] [bar] and [low] [calorie][protien\n",
      "bar] are not, as segments like [low calorie] and [calorie pro-\n",
      "tein] greatly deviate from the intended meaning of the query.\n",
      "Ideally, each segment should map to exactly one concept.\n",
      "For segments like [low calorie prorein bar], the answer of\n",
      "whether it should be left intact as a compound concept or\n",
      "further segmented into multiple atomic concepts depends on\n",
      "the connection strength of the components (i.e. how strong\n",
      "/ often are low calorie and protein bar associated).This\n",
      "leaves some ambiguity in query segmentation, as we will\n",
      "discuss later. Query segmentation is used here as a step to-\n",
      "wards query annotation where we annote the segmentation\n",
      "rather than the query directly. Query segmentation should\n",
      "also be useful in other query processing tasks such as query\n",
      "rewriting and query log analysis, where one would be able\n",
      "to work on semantic concepts instead of individual words.\n",
      "\n",
      "We use ngram interpolation to combine ngram models and to\n",
      "get the segment probability. And also used the EM method\n",
      "to estimate the weight for the interpolation.\n",
      "\n",
      "2.2 Model and Notation\n",
      "Let query q = w1w2 . . . wn where wi denotes the ith word of\n",
      "the query, we let sik = wiwi+1 . . . wi+k1 denotes a segment\n",
      "of the query with k subsequent words starting with the ith\n",
      "word.\n",
      "\n",
      "Segmentation S is a function S(q) = [si1k1 , si2k1 , . . . , simkm ]\n",
      "\n",
      "that splits query q intom consequtive disjoint segments,i.e.,i1 =\n",
      "1,i2 = i1 + k1,. . . ,im = im1 + km1 and im + km = n\n",
      "\n",
      "R(A1, A2, . . . , An) denotes a relation R with attributes A1,\n",
      "A2, . . . , An and A denotes set of all attributes.\n",
      "\n",
      "We define annotation function f(s)  A that returns the\n",
      "attribute correspoding to segment s\n",
      "\n",
      "2.3 Algorithm for query segmentation\n",
      "For two segmentation S1 and S2 of the same query suppose\n",
      "they differ at only the one segment boundry,i.e.,\n",
      "S1 = si1k1 , si2k1 , . . . , si(l1)k(l1), silkl , si(l+1)k(l+1),\n",
      "si(l+2)k(l+2), . . . , simkm\n",
      "\n",
      "S2 = si1k1 , si2k1 , . . . , si(l1)k(l1), s\n",
      "\n",
      "ilkl\n",
      "\n",
      ", si(l+2)k(l+2),\n",
      ". . . , simkm\n",
      "where s\n",
      "\n",
      "\n",
      "ilkl\n",
      "\n",
      "= silkl , si(l+1)k(l+1) is the concatenation of silkl\n",
      "and si(l+1)k(l+1) That means we will favour segmentations\n",
      "\n",
      "with higher probability of generating the query. So in above\n",
      "the language model will have higher probability for P (S1)\n",
      "\n",
      "than P (S2) if and ony if P (silkl , si(l+1)k(l+1)) > P (s\n",
      "\n",
      "ilkl\n",
      "\n",
      ")\n",
      "\n",
      "This is how the algorithm looks like for segmentation given\n",
      "a query\n",
      "\n",
      "Algorithm 1 Dynamic programming algorithm using lan-\n",
      "guage model for k segmenations\n",
      "\n",
      "Input:query w1w2 . . . wn,language model L,number of seg-\n",
      "mentation k\n",
      "Output:top k segmentations with highest likelihood\n",
      "B[i]:top k segmentations for sub-query w1w2 . . . wi\n",
      "For each segmenation b in B[i],segs denotes the segments and\n",
      "prob denotes the probability of the segment in the language\n",
      "model\n",
      "\n",
      "for all i  [1 . . . n] do\n",
      "s w1w2 . . . wi\n",
      "if l.P rob(s) > 0 then\n",
      "a newSegmentation\n",
      "a.segs s\n",
      "a.prob l.P rob(s)\n",
      "end if\n",
      "for all j  [1 . . . i 1] do\n",
      "for all b  B[j] do\n",
      "s wjw(j + 1) . . . wi\n",
      "if l.P rob(s) > 0 then\n",
      "a newSegmentation\n",
      "a.segs b.segss\n",
      "a.prob b.prob  l.P rob(s)\n",
      "B[i] b[i]a\n",
      "end if\n",
      "end for\n",
      "end for\n",
      "sort B[i] by prob\n",
      "truncate B[i] to size k\n",
      "end for\n",
      "\n",
      "2.4 Maximum Entropy Classifier for Annota-\n",
      "tion\n",
      "\n",
      "The goal of statistical modeling is to construct a model that\n",
      "best accounts for some training data. Starting from a set of\n",
      "\n",
      "\n",
      "\n",
      "data such algorithms can automatically extract a set of rela-\n",
      "tionships inherent in the data, and then combine these rules\n",
      "into a model of the data. Maximum Entropy (ME) model\n",
      "is one of the frameworks suggested for this purpose (Ratna-\n",
      "parkhi, 1997). In other words, given an empirical probability\n",
      "distribution, we need to build a model as close to this dis-\n",
      "tribution as possible. Furthermore, the chosen model being\n",
      "consistent with all the facts should be otherwise as uniform\n",
      "as possible. In order to build such a model, a large num-\n",
      "ber of samples (x1, y1), (x2, y2), . . . , (xN , yN ), where x rep-\n",
      "\n",
      "resents a given context and y aAS an outcome, needs to be\n",
      "collected. Then we can summarize the training sample in\n",
      "terms of its probability distribution p (x, y), defined as a\n",
      "number of times the sample (x, y) occurs in the data divided\n",
      "by the overall number of samples in the data. The goal is\n",
      "to choose an optimal model incorporating a set of statistics\n",
      "from the training data (Berger et al., 1996). To do this a\n",
      "set of features that describes the data in the most efficient\n",
      "way needs to be defined. Once a set of features is chosen, we\n",
      "can construct the corresponding ME model by adding fea-\n",
      "tures as constraints to the model and adjust weights of these\n",
      "features. Intuitively, models with high entropy are more uni-\n",
      "form and therefore they assume less about the world. The\n",
      "maximum entropy model can be interpreted as the model\n",
      "that assumes only the knowledge that is represented by the\n",
      "features derived from the training data, and nothing else.\n",
      "\n",
      "The maximum entropy framework estimates probabilities\n",
      "based on the principle of making as few assumptions as pos-\n",
      "sible, other than the constraints imposed. Such constraints\n",
      "are derived from training data, expressing some relationship\n",
      "between features and outcome. The probability distribution\n",
      "that satisfies the above property is the one with the highest\n",
      "entropy. It is unique, agrees with the maximum-likelihood\n",
      "distribution, and has the exponential form (Della Pietra et\n",
      "al., 1997):\n",
      "\n",
      "p(o|h) = 1\n",
      "Z(h)\n",
      "\n",
      "k\n",
      "j=1\n",
      "\n",
      "\n",
      "fj(h,o)\n",
      "\n",
      "j (1)\n",
      "\n",
      "where o refers to outcome, h the history(or context),Z(h)\n",
      "is a normalization function. Each feature fj(h, o) is a bi-\n",
      "nary function.For example, in predicting if a word belongs\n",
      "to a word class,o is either true or false, and h refers to the\n",
      "surrounding context:\n",
      "\n",
      "fj(h, o) =\n",
      "\n",
      "{\n",
      "1 : if = true, previous word = the\n",
      "0 : otherwise\n",
      "\n",
      "The proposed approach consists of training a Maximum En-\n",
      "tropy classifier for the task in question, using a large auto-\n",
      "matically created food related corpus, as opposed to other\n",
      "systems being trained on expensive manually annotated data.\n",
      "First, from database we tag terms belonging to database and\n",
      "train the Maximum Entropy Classifier with it.\n",
      "\n",
      "The basic improvement comes from applying language model\n",
      "for segmentation that we plug in the name entity recognition\n",
      "algorithm using Max Entropy Classifier, any other classifier\n",
      "could be used at this point, logistic regression, maximum\n",
      "likelihood classifier etc. So,as expected,maximum entropy\n",
      "classifier trained on automatically created data and with\n",
      "previously segmented query performs magically better than\n",
      "the baseline recognizer itself, which proves the efficiency of\n",
      "the adopted approach.\n",
      "\n",
      "Algorithm 2 ComputeAnnotation\n",
      "\n",
      "Input:List of Segmentation\n",
      "Output:AnnotatedSegmentation\n",
      "B[i] is a List of Segmentation each with segments having\n",
      "possible top K segmentation from the query using algorithm\n",
      "1\n",
      "\n",
      "for all b  B[i] do\n",
      "aSs \n",
      "for all seg  b do\n",
      "lP = maxentClassifier.getLP (seg)\n",
      "aS  newAnnotatedSegment(seg, lP [0], lP [1])\n",
      "aSs aSsaS\n",
      "end for\n",
      "end for\n",
      "\n",
      "3. SUPERVISED LEARNING APPROACH\n",
      "This is like classic named entity recognition(NER) problem\n",
      "in natural language processing. This involves query labeling\n",
      "and training with the labelled queries. NER systems have\n",
      "been created that use linguistic grammar-based techniques\n",
      "as well as statistical models. Hand-crafted grammar-based\n",
      "systems typically obtain better precision, but at the cost of\n",
      "lower recall and months of work by experienced computa-\n",
      "tional linguists. Statistical NER systems typically require\n",
      "a large amount of manually annotated training data. Sta-\n",
      "tistical NERs usually find the sequence of tags p(N |S)that\n",
      "maximizes the probability , where S is the sequence of words\n",
      "in a sentence, and N is the sequence of named-entity tags\n",
      "assigned to the word in S\n",
      "\n",
      "Supervised learning methods represents linguistic informa-\n",
      "tion in the form of features. Each feature informs of the\n",
      "occurrence of certain attribute in a context that contains a\n",
      "linguistic ambiguity. That context is the text surrounding\n",
      "this ambiguitiy and relevant to the disambiguation process.\n",
      "The features used can be of distinct nature: word colloca-\n",
      "tions, part-of-speech labels, keywords, topics and domain\n",
      "information, etc. A method using supervised learning tries\n",
      "to classify a context containing an ambiguous word or com-\n",
      "pound word in one of its possible senses by means of a clas-\n",
      "sification function. This function is obtained after a training\n",
      "process on a sense tagged corpus. The information source\n",
      "for this training is the set of results of the features evalua-\n",
      "tion on each context, that is, each context has its vector of\n",
      "feature values. The supervised learning method (Maximum\n",
      "Entropy) used in this paper to do such analysis is based on\n",
      "Maximum Entropy Markov Model (MEMM).\n",
      "\n",
      "3.1 Query Labeling\n",
      "The problem with supervised learning is that a huge corpus\n",
      "has to be annotated manually and using that the proba-\n",
      "bilistic classifier will learn how to guess the annotation of\n",
      "unseen example using viterbi algorithm exploiting MEMM.\n",
      "We used an algorithm to annotate data automatically as if\n",
      "they were manually annotated. We assumed a form of the\n",
      "search query for it. First from the strcutured data we pick\n",
      "a row and then flip a coin to decide whether to use that line\n",
      "for training or not.If its selected the next step is to use ei-\n",
      "ther productName,typeName or brandName. Then we flip\n",
      "a coin again to decide whether to do two more annotation\n",
      "or one. Whether its one or two its randomly chosen from\n",
      "\n",
      "\n",
      "\n",
      "ingredients,nutrients,allergens.\n",
      "\n",
      "4. UNSUPERVISED APPROACH\n",
      "This is the approach that we came up with for named entity\n",
      "recognition in a particular context of structured data. Here\n",
      "the annotation is done automatically instead of manually,\n",
      "where each column represents one named entity and thus an\n",
      "algorithm automatically annotes the entry in database later\n",
      "used for training the probability model. At the same time\n",
      "these touples of named entity are used for language model\n",
      "training as well where we ignore the named tag and consider\n",
      "each of the named entity as a sentence for the purpose of\n",
      "training the language model.\n",
      "\n",
      "The following algorithm deals with how to create annotated\n",
      "named entity from structured data automatically.\n",
      "\n",
      "Algorithm 3 Making annotated training data\n",
      "\n",
      "Input:Structured Data\n",
      "Output:List of Pair of sentence and its annota-\n",
      "tion\n",
      "\n",
      "fieldNames fileheader|columnheader\n",
      "for all rowinstructured data do\n",
      "line row\n",
      "field line splitted to field\n",
      "for all f  field do\n",
      "subfield f splitted to subfield\n",
      "for all s insubfield do\n",
      "words each subfield splitted\n",
      "fieldName fieldNames.get(f.index)\n",
      "sentence (words, fieldName)\n",
      "end for\n",
      "end for\n",
      "end for\n",
      "\n",
      "After the named entity has been annotated we can use this\n",
      "existing data structures first element which is the entity to\n",
      "get the sentences that we will need to train the language\n",
      "model. The algorithm is as follows.\n",
      "\n",
      "Algorithm 4 Making training ngram model for segmenta-\n",
      "tion\n",
      "Input:Annotated training data,n\n",
      "Output:Trained language model\n",
      "\n",
      "sentencess annotatedData.words\n",
      "lmodel  new InterpolationMode(n)\n",
      "lmodel.weight weight found by EM\n",
      "lmodel.train(sentences)\n",
      "\n",
      "4.1 Query Segmentation\n",
      "Use dynamic programming algorithm of [11] to do the seg-\n",
      "mentation using the probabilities that we obtain from the\n",
      "trained language models.However we modified this algorithm\n",
      "in 2.3 to consider the sentence probability using language\n",
      "model instead of calculating the sentence probability using\n",
      "individual probability of words. In this way the free words\n",
      "are taken care of automatically and no additional algorithm\n",
      "is needed to handle the free words. After we ran experient\n",
      "we figured having background knowledge on ngram language\n",
      "\n",
      "model improves the dynamic programic algortihm from its\n",
      "original version.\n",
      "\n",
      "4.2 Segment Classification\n",
      "We use maxEnt classifier to do classification of the already\n",
      "segmented query, that is on the segmentation. The advan-\n",
      "tage here is we donot need to look at the whole query. When\n",
      "estimating the annotation only the segment words are looked\n",
      "at and the context to other segments is not necessary. We\n",
      "use 2.4 to find the segment annotation.\n",
      "\n",
      "5. EXPERIMENTS AND EVALUATION\n",
      "The database has a relation with 30,000 records and 10 at-\n",
      "tributes. We used a test set of 100 manually labelled queries.\n",
      "These data are the structured data of www.caloricious.com.\n",
      "The manually labeled query are from the server logs which\n",
      "stores the actual queries entered by humans. First we ran us-\n",
      "ing the traditional NER which uses MEMM to annotate. We\n",
      "synthetically generated annotated data from corpus which\n",
      "is as good as annotating manually.Lets discuss the struc-\n",
      "tured data before going to the detail of how we generated\n",
      "the synthetical data its logical to discuss the data format in\n",
      "the structured data for caloricious. It has few column heads\n",
      "called product name,type name,brand name,ingredients nu-\n",
      "trients ,allergens. Examples of each of the type are prod-\n",
      "uct name( united bakery salted breadsticks, country white\n",
      "bagels, lemon sorbet), type name( milkshakes, dressings,\n",
      "granola bars), brand name ( hersheys, newtree, ritter sport),\n",
      "ingredients ( sugar, cocoa butter, organic almond butter),\n",
      "nutrients ( vitamin b5, citric acid),allergens( low total fat,\n",
      "low sodium, gluten free). The algorithm discussed in 3.1\n",
      "is used to create annotated training data. Its assumed that\n",
      "users would have either product name, type name or brand\n",
      "name with some of the ingredients, nutrients or allergians,\n",
      "its all randomized to generate differnt combination. We used\n",
      "both this approach and annotating all the existing data row\n",
      "assuming each row to be a long query. The later took a long\n",
      "time to train the MEMM model because of its size however\n",
      "the former was very quick to generate the model as corpus\n",
      "was smaller. However, the later was 53% accurate whereas\n",
      "the former was 49% accurate. This tells that if the corpus is\n",
      "really huge and annotating every datum is not plausible our\n",
      "smart query generator can be used as a reasonable compro-\n",
      "mise for computational compelxity and still generate a good\n",
      "result.Figure 5 shows the comparison between these two.\n",
      "\n",
      "For the unsupervised and our proposed approach we ran on\n",
      "the same data set and there were 6093070 sentences as we\n",
      "made each of the entity in database a sentence. We first\n",
      "tried using unigram model and the accuracy came upto 47%\n",
      "which is just a little(4%) below the MEMM model for an-\n",
      "notated data. Which basically shows the power of language\n",
      "model over annotation. Because even though the accuracy is\n",
      "not higher for unigram, it tells that annotation usually have\n",
      "to be on huge data to make it efficient, because identifying\n",
      "unkown entity gets difficult for supervised learning. In 5\n",
      "we show the NER for different entity and its accuracy com-\n",
      "paring to the orignal and correctly annotated ratio.It does\n",
      "reasonable on ingredients which tells that identifying ingre-\n",
      "dients is easier in general and the feature identification works\n",
      "for ingredients. Next we try to use bigram for segmentation\n",
      "and feed this to our annotation model. Not surprisingly it\n",
      "performs very good and give 82% accuracy. We used interpo-\n",
      "\n",
      "\n",
      "\n",
      "Figure 1: Accuracy using unigram,showing ratio of\n",
      "annotated and real entity\n",
      "\n",
      "Figure 2: Accuracy using bigram,showing ratio of\n",
      "annotated and real entity\n",
      "\n",
      "Figure 3: Accuracy using trigram,showing ratio of\n",
      "annotated and real entity\n",
      "\n",
      "lation for bigram and used EM model to calculate the weight\n",
      "for the model which came up to 0.010394140437218138 for\n",
      "unigram and 0.9896058595627819 for bigram. Fig 5 show\n",
      "the performance of bigram on each of the different named\n",
      "entity.\n",
      "\n",
      "After bigram, we used trigram for the same purpose of doing\n",
      "\n",
      "fig:sup\n",
      "\n",
      "Figure 4: Overall accuracy comparison using uni-\n",
      "gram,bigram,trigram and supervised learning using\n",
      "MEMM\n",
      "\n",
      "Figure 5: Accuracy using smart synthetic data vs\n",
      "annoting the whole corpus\n",
      "\n",
      "query segmentation for feeding it to the annotation system.\n",
      "To our little surprise bigram performed better and trigram\n",
      "gave 79.23% accuracy. For trigram model we used interpo-\n",
      "lation the weight of which is decided using EM algorithm\n",
      "which came up as 0.04033219974762, 0.160914763422446,\n",
      "0.7987530368299335 for unigram,bigram,trigram respectively.\n",
      "What it tells is basically queries are short and the named\n",
      "entity are usually easily identified in two terms or bigram\n",
      "model rather than trigram. So its a good indication why\n",
      "higher model ngram wouldnt be necessary for query segmen-\n",
      "tation.Fig 5 show the performance of bigram on each of the\n",
      "different named entity.\n",
      "\n",
      "So the experiment shows clearly how brininging in languae\n",
      "model dramatically improves the performance of query la-\n",
      "belling. Its a combination of query segmentation and use of\n",
      "language model that made our algorithm to perform really\n",
      "well.\n",
      "\n",
      "6. RELATED WORK\n",
      "We are essentially solving two problems and combining them\n",
      "to solve an interesting problem of query segmentation and\n",
      "annotation. Its combining query segmentaiton and apply\n",
      "annotation on top of it. In natural language processing,\n",
      "\n",
      "\n",
      "\n",
      "there has been a significant amount of research on text seg-\n",
      "mentation, such as noun phrase chunking [[12]], where the\n",
      "task is to recognize the chunks that consist of noun phrases,\n",
      "and Chinese word segmentation [[1]], where the task is to\n",
      "delimit words by putting boundaries between Chinese char-\n",
      "acters. Query segmentation is similar to these problems in\n",
      "the sense that they all try to identify meaningful seman-\n",
      "tic units from the input. However, one may not be able to\n",
      "apply these techniques directly to query segmentation, be-\n",
      "cause Web search query language is very different (queries\n",
      "tend to be short, composed of keywords), and some essential\n",
      "techniques to noun phrase chunking, such as partof- speech\n",
      "tagging [[2]], can not achieve high performance when applied\n",
      "to queries.\n",
      "\n",
      "Interestingly, there is little investigation done of query seg-\n",
      "mentation,let along its use in named entity recognition in\n",
      "spite of its importance. To our knowledge, two approaches\n",
      "have been studied in previous works. The first is based on\n",
      "(pointwise) mutual information (MI) between pairs of query\n",
      "words. If the MI value between two adjacent words is be-\n",
      "low a predefined threshold, a segment boundary is inserted\n",
      "at that position. The problem with this approach is that\n",
      "MI, by definition, cannot capture correlations among more\n",
      "than two words, thus it cannot handle long entities like song\n",
      "names, where MI may be low in certain positions ( e.g., be-\n",
      "\n",
      "tween heart and will in aAIJmy heart will go onaAI).In\n",
      "our approach MI is used between two segments, not two\n",
      "words which is more relevent on the background of languge\n",
      "model.\n",
      "\n",
      "A problem related to generating plausible structured anno-\n",
      "tations, referred to as web query tagging, was introduced\n",
      "in[7] . Its goal is to assign each query term to a specified\n",
      "category, roughly corresponding to a table attribute. A Con-\n",
      "ditional Random Field (CRF) is used to capture dependen-\n",
      "cies between query words and identify the most likely joint\n",
      "assignment of words to aAIJcategoriesaAI.Keyword search\n",
      "on relational [[5], [8], [6]], semi-structured [[3],[9]] and graph\n",
      "data [ [4]] (Keyword Search Over Structured Data, abbre-\n",
      "viated as KSOSD) has been an extremely active research\n",
      "topic. Its goal is the efficient retrieval of relevant database\n",
      "tuples, XML sub-trees or subgraphs in response to keyword\n",
      "queries. The problem is challenging since the relevant pieces\n",
      "of information needed to assemble answers are assumed to be\n",
      "scattered across relational tables, graph nodes, etc. Essen-\n",
      "tially, KSOSD techniques allow users to formulate compli-\n",
      "cated join queries against a database using keywords. The\n",
      "tuples returned are ranked based on the distance in the\n",
      "database of the fragments joined to produce a tuple, and\n",
      "the textual similarity of the fragments to query terms. In\n",
      "[10] the authors study the state of art of the problem we\n",
      "tried to solve but with respect to web queries. It has funda-\n",
      "mental limitations of being very complex and doing EM on\n",
      "the fly all the time which is not feasible for product searches.\n",
      "\n",
      "7. CONCLUSIONS\n",
      "We have proved that query annotation works almost dra-\n",
      "matically better in the new approach we came up with of\n",
      "segmenting the query before and then applying annotaiton\n",
      "having a language model as the base of segmentation. We\n",
      "have used a real exisitng website www.caloricious.com for\n",
      "our experiment and proved our algorithms superiority over\n",
      "\n",
      "the existing algorithm. Our future plan is to run the algo-\n",
      "rithm on other product websites data and see whether the\n",
      "some performance holds and eventually come up with a do-\n",
      "main indpendent model for query segmentation.\n",
      "\n",
      "8. REFERENCES\n",
      "[1] M. R. Brent and T. A. Cartwright. Distributional\n",
      "\n",
      "regularity and phonotactic constraints are useful for\n",
      "segmentation. volume 61, pages 93125, 1996.\n",
      "\n",
      "[2] E. Brill and G. Ngai. Man vs. machine: A case study\n",
      "in base noun phrase learning. 1999.\n",
      "\n",
      "[3] L. Guo, F. Shao, C. Botev, and\n",
      "J. Shanmugasundaram. Xrank: ranked keyword search\n",
      "over xml documents. In Proceedings of the 2003 ACM\n",
      "SIGMOD international conference on Management of\n",
      "data, SIGMOD 03, pages 1627, New York, NY,\n",
      "USA, 2003. ACM.\n",
      "\n",
      "[4] H. He, H. Wang, J. Yang, and P. S. Yu. Blinks:\n",
      "ranked keyword searches on graphs. In Proceedings of\n",
      "the 2007 ACM SIGMOD international conference on\n",
      "Management of data, SIGMOD 07, pages 305316,\n",
      "New York, NY, USA, 2007. ACM.\n",
      "\n",
      "[5] V. Hristidis, L. Gravano, and Y. Papakonstantinou.\n",
      "Efficient ir-style keyword search over relational\n",
      "databases. In Proceedings of the 29th international\n",
      "conference on Very large data bases - Volume 29,\n",
      "VLDB 2003, pages 850861. VLDB Endowment, 2003.\n",
      "\n",
      "[6] E. Kandogan, R. Krishnamurthy, S. Raghavan,\n",
      "S. Vaithyanathan, and H. Zhu. Avatar semantic\n",
      "search: a database approach to information retrieval.\n",
      "In Proceedings of the 2006 ACM SIGMOD\n",
      "international conference on Management of data,\n",
      "SIGMOD 06, pages 790792, New York, NY, USA,\n",
      "2006. ACM.\n",
      "\n",
      "[7] X. Li, Y.-Y. Wang, and A. Acero. Extracting\n",
      "structured information from user queries with\n",
      "semi-supervised conditional random fields. In\n",
      "Proceedings of the 32nd international ACM SIGIR\n",
      "conference on Research and development in\n",
      "information retrieval, SIGIR 09, pages 572579, New\n",
      "York, NY, USA, 2009. ACM.\n",
      "\n",
      "[8] F. Liu, C. Yu, W. Meng, and A. Chowdhury. Effective\n",
      "keyword search in relational databases. In Proceedings\n",
      "of the 2006 ACM SIGMOD international conference\n",
      "on Management of data, SIGMOD 06, pages 563574,\n",
      "New York, NY, USA, 2006. ACM.\n",
      "\n",
      "[9] Z. Liu and Y. Cher. Reasoning and identifying\n",
      "relevant matches for xml keyword search. Proc. VLDB\n",
      "Endow., 1:921932, August 2008.\n",
      "\n",
      "[10] N. Sarkas, S. Paparizos, and P. Tsaparas. Structured\n",
      "annotations of web queries. In Proceedings of the 2010\n",
      "international conference on Management of data,\n",
      "SIGMOD 10, pages 771782, New York, NY, USA,\n",
      "2010. ACM.\n",
      "\n",
      "[11] B. Tan and F. Peng. Unsupervised query segmentation\n",
      "using generative language models and wikipedia. In\n",
      "Proceeding of the 17th international conference on\n",
      "World Wide Web, WWW 08, pages 347356, New\n",
      "York, NY, USA, 2008. ACM.\n",
      "\n",
      "[12] C. Zhang, N. Sun, X. Hu, T. Huang, and T. seng\n",
      "Chua. Query segmentation based on eigenspace\n",
      "similarity.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_text_lines = file_text.splitlines()\n",
    "for line in file_text_lines:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
