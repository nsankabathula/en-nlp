{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import autocorrect\n",
    "import spacy\n",
    "\n",
    "docs = [\n",
    "    'He playeed football',\n",
    "    'He plays cricket',\n",
    "    'He had sandwich for dinner'\n",
    "]\n",
    "'''\n",
    "docs = [\n",
    "    'new york times',\n",
    "    'new york post',\n",
    "    'los angeles times'\n",
    "]\n",
    "\n",
    "'''\n",
    "\n",
    "class SpellTokenizer(object):\n",
    "    \n",
    "    def __init__(self, nlp):\n",
    "        self.vocab = nlp.vocab\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        doc = nlp.tokenizer(text)\n",
    "        words = [autocorrect.spell(i.orth_) for i in doc]\n",
    "        return spacy.tokens.Doc(self.vocab, words = words)\n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.make_doc = SpellTokenizer(nlp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokens(doc, lemma = False):\n",
    "    s_doc = nlp(doc)\n",
    "    tokens = []\n",
    "    for token in s_doc:\n",
    "        if(lemma):\n",
    "        #print(token, token.lemma_, token.pos_)\n",
    "            if(token.lemma_ == 'PRON' or token.lemma_ == '-PRON-'):\n",
    "                tokens.append(token.lower_)\n",
    "            else:\n",
    "                tokens.append(token.lemma_)  \n",
    "        else:\n",
    "            tokens.append(token.lower_)\n",
    "    #print (tokens)\n",
    "    return tokens\n",
    "\n",
    "def Vocab(tokens):\n",
    "    alltokens = []\n",
    "    for doc_token in tokens:\n",
    "        alltokens = alltokens + doc_token        \n",
    "    return set(alltokens)\n",
    "\n",
    "def Dict(tokens, vocab):\n",
    "    docsDict = []\n",
    "    docDict = dict.fromkeys(vocab, 0)     \n",
    "    for  doc_token  in  tokens:\n",
    "        docDict = dict.fromkeys(vocab, 0)    \n",
    "        for word in doc_token:        \n",
    "            docDict [word]+=1\n",
    "        docsDict.append(docDict)\n",
    "    return docsDict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [Tokens(doc, True) for doc in docs]\n",
    "vocab = Vocab(tokens)\n",
    "docsDict = Dict(tokens, vocab)\n",
    "pd.DataFrame(docsDict, index = docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(docsDict, tokens):\n",
    "    tfDicts = []\n",
    "    for idx in range(len(tokens)):\n",
    "        doc_token = tokens[idx]\n",
    "        wordDict = docsDict[idx]\n",
    "        tfDict = {}\n",
    "        bowCount = len(doc_token)\n",
    "        for word, count in wordDict.items():\n",
    "            tfDict[word] = count/float(bowCount)\n",
    "        tfDicts.append(tfDict)\n",
    "    return tfDicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsTF = computeTF(docsDict, tokens)\n",
    "pd.DataFrame(docsTF, index = docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDFList(docList):\n",
    "    import math\n",
    "    docsIdfDict = []\n",
    "    N = len(docList)\n",
    "    \n",
    "    for doc in docList:\n",
    "        idfDict = dict.fromkeys(doc.keys(), 0)\n",
    "        #print(idfDict, doc.items())    \n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "        \n",
    "        #print(idfDict.items())   \n",
    "        for word, val in idfDict.items():\n",
    "            idfDict[word] = math.log2(N / float(val)) if(val > 0) else 0\n",
    "        #print(idfDict.items())     \n",
    "        docsIdfDict.append(idfDict)\n",
    "    \n",
    "    return docsIdfDict\n",
    "\n",
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            #print(word, val)\n",
    "            if val > 0:\n",
    "                idfDict[word] += val\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val), 2)\n",
    "        #print (word, idfDict[word], val)\n",
    "    \n",
    "    return idfDict\n",
    "\n",
    "docsIDF = computeIDF(docsTF)\n",
    "pd.DataFrame(docsIDF, index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(docsTF, idfs):\n",
    "    docsTFDIF = []\n",
    "    for idx, tfBow in enumerate(docsTF) :\n",
    "        tfidf = {}        \n",
    "        for word, val in tfBow.items():            \n",
    "            tfidf[word] = val*idfs[word]\n",
    "        docsTFDIF.append(tfidf)\n",
    "    return docsTFDIF\n",
    "\n",
    "docsTFDIF = computeTFIDF(docsTF, docsIDF)\n",
    "pd.DataFrame(docsTFDIF, index = docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
